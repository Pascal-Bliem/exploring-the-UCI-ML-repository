{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction on colonoscopy video data \n",
    "\n",
    "Machine learning is becoming more successful in many medical applications, especially in analyzing image or video data. The field of proctology is no exception. I find this example particularly interesting as one of my best friends has to get regular colonoscopies as a measure of colon cancer prophylactics and I'd like to tell her if machine learning can assist the doctors in classifying whatever they find in her behind as benign or malignant.\n",
    "\n",
    "This data set contains 76 instances of gastrointestinal lesions, two malignant and one benign type, from regular colonoscopy data. It was contributed to the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Gastrointestinal+Lesions+in+Regular+Colonoscopy) by Pablo Mesejo and Daniel Pizarro. You can find a detailed description on the website. They also provided a summary of how human experts performed on classifying the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame the Problem\n",
    "In this example, we want to classify each lesions as either serrated adenomas, adenoma, or hyperplastic lesion. The first two are considered malignant, the latter as benign. Hence, we can also treat it as a binary classification problem (the authors of the data set labeled this binary case as *resection* vs. *no-resection*). \n",
    "\n",
    "What is special about this data set is that we are dealing with a lot of features but only a few instances. From each video, there are 422 2D textural features, 76 2D color features, 200 3D shape features, and each of those recorded under two different light conditions. A total of 1397 features and only 76 instances. Most classification algorithms will have problems with this ratio. Support vector machines are known for being able to deal with many features given few instances, but this ratio might be too extreme even for them. So what can we do about it? \n",
    "\n",
    "In this example we will try to apply principal component analysis (PCA) to reduce the dimensionality of the feature space while still preserving much information of the original data. PCA will project the original high-dimensional data on a lower dimensional hyperplane (which is spanned by the principal components). We can either specify a number of dimensions and pick the PCs that preserve the most variance or we specify ho much variance should be preserved and pick the number of PCs accordingly. \n",
    "\n",
    "I would usually chose recall as optimization metric because I think it is important to not miss potential cancer; but since we want to compare to human performance (who probably want to get all classifications right) and we have a class imbalance, I'll choose the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import numpy as np # numerical computation\n",
    "import pandas as pd # data handling\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# visulalization\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "Let's import the data into data frame hand have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adenoma_1</th>\n",
       "      <th>adenoma_1.1</th>\n",
       "      <th>adenoma_8</th>\n",
       "      <th>adenoma_8.1</th>\n",
       "      <th>adenoma_9</th>\n",
       "      <th>adenoma_9.1</th>\n",
       "      <th>adenoma_10</th>\n",
       "      <th>adenoma_10.1</th>\n",
       "      <th>adenoma_11</th>\n",
       "      <th>adenoma_11.1</th>\n",
       "      <th>...</th>\n",
       "      <th>serrated_5</th>\n",
       "      <th>serrated_5.1</th>\n",
       "      <th>serrated_6</th>\n",
       "      <th>serrated_6.1</th>\n",
       "      <th>serrated_7</th>\n",
       "      <th>serrated_7.1</th>\n",
       "      <th>serrated_8</th>\n",
       "      <th>serrated_8.1</th>\n",
       "      <th>serrated_9</th>\n",
       "      <th>serrated_9.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138.120000</td>\n",
       "      <td>127.990000</td>\n",
       "      <td>80.415000</td>\n",
       "      <td>90.896000</td>\n",
       "      <td>106.160000</td>\n",
       "      <td>147.090000</td>\n",
       "      <td>148.730000</td>\n",
       "      <td>126.050000</td>\n",
       "      <td>109.130000</td>\n",
       "      <td>129.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>114.590000</td>\n",
       "      <td>86.424000</td>\n",
       "      <td>163.680000</td>\n",
       "      <td>71.638000</td>\n",
       "      <td>180.110000</td>\n",
       "      <td>136.550000</td>\n",
       "      <td>96.852000</td>\n",
       "      <td>157.810000</td>\n",
       "      <td>93.569000</td>\n",
       "      <td>95.543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1606.800000</td>\n",
       "      <td>3377.900000</td>\n",
       "      <td>1852.100000</td>\n",
       "      <td>1904.300000</td>\n",
       "      <td>1184.400000</td>\n",
       "      <td>822.320000</td>\n",
       "      <td>2412.500000</td>\n",
       "      <td>4752.200000</td>\n",
       "      <td>999.390000</td>\n",
       "      <td>599.950000</td>\n",
       "      <td>...</td>\n",
       "      <td>3014.500000</td>\n",
       "      <td>3500.900000</td>\n",
       "      <td>3253.100000</td>\n",
       "      <td>1822.200000</td>\n",
       "      <td>1198.500000</td>\n",
       "      <td>1316.300000</td>\n",
       "      <td>2071.300000</td>\n",
       "      <td>2732.300000</td>\n",
       "      <td>1163.600000</td>\n",
       "      <td>2240.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003875</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.004188</td>\n",
       "      <td>0.005541</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>0.004148</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>0.004379</td>\n",
       "      <td>0.004015</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.004803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     adenoma_1  adenoma_1.1    adenoma_8  adenoma_8.1    adenoma_9  \\\n",
       "0     3.000000     3.000000     3.000000     3.000000     3.000000   \n",
       "1     1.000000     2.000000     2.000000     1.000000     2.000000   \n",
       "2   138.120000   127.990000    80.415000    90.896000   106.160000   \n",
       "3  1606.800000  3377.900000  1852.100000  1904.300000  1184.400000   \n",
       "4     0.003875     0.003564     0.004761     0.004147     0.005518   \n",
       "\n",
       "   adenoma_9.1   adenoma_10  adenoma_10.1  adenoma_11  adenoma_11.1  ...  \\\n",
       "0     3.000000     3.000000      3.000000    3.000000      3.000000  ...   \n",
       "1     1.000000     1.000000      2.000000    2.000000      1.000000  ...   \n",
       "2   147.090000   148.730000    126.050000  109.130000    129.700000  ...   \n",
       "3   822.320000  2412.500000   4752.200000  999.390000    599.950000  ...   \n",
       "4     0.003871     0.003336      0.004188    0.005541      0.005917  ...   \n",
       "\n",
       "    serrated_5  serrated_5.1   serrated_6  serrated_6.1   serrated_7  \\\n",
       "0     2.000000      2.000000     2.000000      2.000000     2.000000   \n",
       "1     1.000000      2.000000     1.000000      2.000000     1.000000   \n",
       "2   114.590000     86.424000   163.680000     71.638000   180.110000   \n",
       "3  3014.500000   3500.900000  3253.100000   1822.200000  1198.500000   \n",
       "4     0.004444      0.003409     0.004869      0.004148     0.003273   \n",
       "\n",
       "   serrated_7.1   serrated_8  serrated_8.1   serrated_9  serrated_9.1  \n",
       "0      2.000000     2.000000      2.000000     2.000000      2.000000  \n",
       "1      2.000000     2.000000      1.000000     1.000000      2.000000  \n",
       "2    136.550000    96.852000    157.810000    93.569000     95.543000  \n",
       "3   1316.300000  2071.300000   2732.300000  1163.600000   2240.500000  \n",
       "4      0.002442     0.004379      0.004015     0.002199      0.004803  \n",
       "\n",
       "[5 rows x 152 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colon = pd.read_csv(\"data.txt\")\n",
    "colon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this raw format, each column represents an instance measured at one of the two different light conditions, and the rows represents a features. The first row is the class label and the second row the light condition (1 or 2). We'll first have to merge the two light conditions per instance and than transpose the data frame so that we have the data in a [tidy format](https://vita.had.co.nz/papers/tidy-data.pdf#targetText=This%20paper%20tackles%20a%20small,observational%20unit%20is%20a%20table.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the column names for light conditions 1 and 2\n",
    "light1cols = colon.loc[1,colon.iloc[1]==1].index\n",
    "light2cols = colon.loc[1,colon.iloc[1]==2].index\n",
    "# create seperate data frames for the two conditions\n",
    "light1 = colon[light1cols]\n",
    "light2 = colon[light2cols]\n",
    "# give them the same column names so that they can be appended \n",
    "light2.columns = light1.columns\n",
    "# append data frame while dropping the light condtion and class label from one of them\n",
    "colon = light1.append(light2.iloc[2:])\n",
    "# drop the light condition from the other one\n",
    "colon = colon.drop(1,axis=0).reset_index(drop=True)\n",
    "# transpose the data frame so that instances are rows and features are columns\n",
    "colon = colon.T.reset_index(drop=True).rename(columns={0:\"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1387</th>\n",
       "      <th>1388</th>\n",
       "      <th>1389</th>\n",
       "      <th>1390</th>\n",
       "      <th>1391</th>\n",
       "      <th>1392</th>\n",
       "      <th>1393</th>\n",
       "      <th>1394</th>\n",
       "      <th>1395</th>\n",
       "      <th>1396</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>138.120</td>\n",
       "      <td>1606.80</td>\n",
       "      <td>0.003875</td>\n",
       "      <td>0.005880</td>\n",
       "      <td>0.005213</td>\n",
       "      <td>0.006935</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.009580</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013994</td>\n",
       "      <td>0.013532</td>\n",
       "      <td>0.013157</td>\n",
       "      <td>0.012743</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.012422</td>\n",
       "      <td>0.012252</td>\n",
       "      <td>0.011377</td>\n",
       "      <td>0.011198</td>\n",
       "      <td>0.011131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>90.896</td>\n",
       "      <td>1904.30</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>0.006728</td>\n",
       "      <td>0.005061</td>\n",
       "      <td>0.006879</td>\n",
       "      <td>0.007948</td>\n",
       "      <td>0.009525</td>\n",
       "      <td>0.010492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.002922</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>0.002531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>147.090</td>\n",
       "      <td>822.32</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>0.005211</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>0.006971</td>\n",
       "      <td>0.011036</td>\n",
       "      <td>0.012802</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>148.730</td>\n",
       "      <td>2412.50</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.007695</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.007721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.020115</td>\n",
       "      <td>0.019595</td>\n",
       "      <td>0.019252</td>\n",
       "      <td>0.018897</td>\n",
       "      <td>0.018177</td>\n",
       "      <td>0.018158</td>\n",
       "      <td>0.017587</td>\n",
       "      <td>0.017109</td>\n",
       "      <td>0.016648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>129.700</td>\n",
       "      <td>599.95</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.007934</td>\n",
       "      <td>0.006976</td>\n",
       "      <td>0.007695</td>\n",
       "      <td>0.008404</td>\n",
       "      <td>0.008825</td>\n",
       "      <td>0.010306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1397 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label        1        2         3         4         5         6         7  \\\n",
       "0    3.0  138.120  1606.80  0.003875  0.005880  0.005213  0.006935  0.007333   \n",
       "1    3.0   90.896  1904.30  0.004147  0.006728  0.005061  0.006879  0.007948   \n",
       "2    3.0  147.090   822.32  0.003871  0.005211  0.005834  0.006971  0.011036   \n",
       "3    3.0  148.730  2412.50  0.003336  0.007695  0.004139  0.005736  0.005794   \n",
       "4    3.0  129.700   599.95  0.005917  0.007934  0.006976  0.007695  0.008404   \n",
       "\n",
       "          8         9  ...      1387      1388      1389      1390      1391  \\\n",
       "0  0.009580  0.007380  ...  0.013994  0.013532  0.013157  0.012743  0.012613   \n",
       "1  0.009525  0.010492  ...  0.003564  0.003380  0.003232  0.003200  0.003006   \n",
       "2  0.012802  0.011083  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.006697  0.007721  ...  0.020822  0.020115  0.019595  0.019252  0.018897   \n",
       "4  0.008825  0.010306  ...  0.000140  0.000129  0.000117  0.000111  0.000106   \n",
       "\n",
       "       1392      1393      1394      1395      1396  \n",
       "0  0.012422  0.012252  0.011377  0.011198  0.011131  \n",
       "1  0.002985  0.002922  0.002631  0.002610  0.002531  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.018177  0.018158  0.017587  0.017109  0.016648  \n",
       "4  0.000098  0.000093  0.000082  0.000079  0.000076  \n",
       "\n",
       "[5 rows x 1397 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that looks a lot tidier. Let's also change the class labels: 1 becomes 0 for benign; 2 and 3 become 1 for malignant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1387</th>\n",
       "      <th>1388</th>\n",
       "      <th>1389</th>\n",
       "      <th>1390</th>\n",
       "      <th>1391</th>\n",
       "      <th>1392</th>\n",
       "      <th>1393</th>\n",
       "      <th>1394</th>\n",
       "      <th>1395</th>\n",
       "      <th>1396</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>127.47</td>\n",
       "      <td>1406.80</td>\n",
       "      <td>0.002802</td>\n",
       "      <td>0.004276</td>\n",
       "      <td>0.003806</td>\n",
       "      <td>0.005146</td>\n",
       "      <td>0.005410</td>\n",
       "      <td>0.006105</td>\n",
       "      <td>0.007818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003110</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.002898</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.002090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>140.73</td>\n",
       "      <td>711.59</td>\n",
       "      <td>0.006186</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>0.006869</td>\n",
       "      <td>0.009844</td>\n",
       "      <td>0.010494</td>\n",
       "      <td>0.012854</td>\n",
       "      <td>0.012929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.0</td>\n",
       "      <td>163.11</td>\n",
       "      <td>1385.90</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.011529</td>\n",
       "      <td>0.006550</td>\n",
       "      <td>0.007291</td>\n",
       "      <td>0.012066</td>\n",
       "      <td>0.013678</td>\n",
       "      <td>0.015943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0</td>\n",
       "      <td>154.63</td>\n",
       "      <td>566.70</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.008225</td>\n",
       "      <td>0.006184</td>\n",
       "      <td>0.008711</td>\n",
       "      <td>0.006867</td>\n",
       "      <td>0.008170</td>\n",
       "      <td>0.013066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>158.80</td>\n",
       "      <td>3795.40</td>\n",
       "      <td>0.005593</td>\n",
       "      <td>0.010721</td>\n",
       "      <td>0.009279</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.014284</td>\n",
       "      <td>0.013749</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1397 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label       1        2         3         4         5         6         7  \\\n",
       "9     1.0  127.47  1406.80  0.002802  0.004276  0.003806  0.005146  0.005410   \n",
       "40    0.0  140.73   711.59  0.006186  0.009827  0.006869  0.009844  0.010494   \n",
       "36    1.0  163.11  1385.90  0.008264  0.011529  0.006550  0.007291  0.012066   \n",
       "47    0.0  154.63   566.70  0.005386  0.008225  0.006184  0.008711  0.006867   \n",
       "22    1.0  158.80  3795.40  0.005593  0.010721  0.009279  0.011931  0.014284   \n",
       "\n",
       "           8         9  ...      1387      1388      1389      1390      1391  \\\n",
       "9   0.006105  0.007818  ...  0.003110  0.003053  0.002898  0.002774  0.002632   \n",
       "40  0.012854  0.012929  ...  0.000002  0.000002  0.000001  0.000001  0.000001   \n",
       "36  0.013678  0.015943  ...  0.000050  0.000048  0.000046  0.000046  0.000044   \n",
       "47  0.008170  0.013066  ...  0.000005  0.000005  0.000005  0.000004  0.000004   \n",
       "22  0.013749  0.018370  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        1392      1393      1394      1395      1396  \n",
       "9   0.002493  0.002450  0.002312  0.002179  0.002090  \n",
       "40  0.000001  0.000001  0.000001  0.000001  0.000001  \n",
       "36  0.000039  0.000033  0.000032  0.000030  0.000027  \n",
       "47  0.000004  0.000003  0.000003  0.000003  0.000003  \n",
       "22  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 1397 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colon.label[colon.label==1] = 0\n",
    "colon.label[(colon.label==2) | (colon.label==3)] = 1\n",
    "colon.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look for missing values\n",
    "colon.isna().sum()[colon.isna().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    55\n",
       "0.0    21\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at class distribution\n",
    "colon.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we don't have to clean the data anymore. We should now split off our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# put 20%  of the data aside as a test set\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1234)\n",
    "for train_index, test_index in split.split(colon, colon[\"label\"]):\n",
    "    colon_train = colon.loc[train_index]\n",
    "    colon_test = colon.loc[test_index]\n",
    "    X_train = colon_train.iloc[:,1:]\n",
    "    y_train = colon_train.loc[:,\"label\"]\n",
    "    X_test = colon_test.iloc[:,1:]\n",
    "    y_test = colon_test.loc[:,\"label\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting and tuning models\n",
    "Let's try to get a base line first by training an SVC model with all the features and no preprocessing besides standard scaling. We can than compare the performance of the lower dimensionality models with this benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation Grid search:\n",
      "Best parameter (CV F1 score=0.880):\n",
      "{'svc__C': 0.1, 'svc__gamma': 1e-08, 'svc__kernel': 'linear'}\n",
      "\n",
      "Performance on the test set:\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 4  0]\n",
      " [ 2 10]]\n",
      "F1: 0.909\n",
      "Recall: 0.833\n",
      "Precision: 1.000\n",
      "Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "# import all sklearn functions needed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# put a standard scaler and a support vector calssifier in a pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "               (\"scaler\",StandardScaler()),\n",
    "               (\"svc\",SVC(class_weight=\"balanced\"))])\n",
    "\n",
    "# define paramter space to search durring grid search\n",
    "param_grid = {\"svc__kernel\":[\"linear\",\"rbf\"],\n",
    "              \"svc__C\":np.logspace(-6,4,5),\n",
    "              \"svc__gamma\":np.logspace(-8,1,5)}\n",
    "\n",
    "# perform grid search\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid,\n",
    "                    n_jobs=-1, cv=6, scoring=\"f1\",\n",
    "                    iid=False, verbose=0)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "print(\"Cross validation Grid search:\")\n",
    "print(\"Best parameter (CV F1 score=%0.3f):\" % grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "print(\"\\nPerformance on the test set:\\n\")\n",
    "print(\"Confusion matrix:\")\n",
    "y_pred = grid.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"F1: {:0.3f}\".format(f1_score(y_test, y_pred)))\n",
    "print(\"Recall: {:0.3f}\".format(recall_score(y_test, y_pred)))\n",
    "print(\"Precision: {:0.3f}\".format(precision_score(y_test, y_pred)))\n",
    "print(\"Accuracy: {:0.3f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually perform already very well without any fancy preprocessing, already beating the human experts who's binary classification accuracy is around 0.796. But we still have a few false negatives in our test set classification which can be a severe problem for malignant tumors. If a patient is told that they're fine, the tumor may get a lot worse until they notice that the diagnosis was wrong. Maybe we can improve our benchmark results.\n",
    "\n",
    "Let's start with a simple PCA with only two PCs so that we can plot our data points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by the first two PCs: 0.921\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# do a PCA with 2 PCs\n",
    "# Note: the infput data is centred aroud 0 automatically before the PCA,\n",
    "# the whiten parameter scales the output to unit variance, so it basically acts\n",
    "# like a standard scaler\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "print(\"Variance explained by the first two PCs: {:0.3f}\".format(sum(pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! 92% of the original training data is preserved when projecting it from 1396 dimensions to only two dimensions. Lets plot it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split instances into malignant and benign\n",
    "idx_b = np.argwhere(y_train==0).reshape(1,-1)\n",
    "idx_m = np.argwhere(y_train==1).reshape(1,-1)\n",
    "malignant = X_train_pca[idx_m].reshape(-1,2)\n",
    "benign = X_train_pca[idx_b].reshape(-1,2)\n",
    "\n",
    "# plot on a scatter plot\n",
    "plt.plot(malignant[:,0],malignant[:,1],\"^r\",label=\"malignant\",ms=8,alpha=.5)\n",
    "plt.plot(benign[:,0],benign[:,1],\".b\",label=\"benign\",ms=10,alpha=.5)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that benign and malignant lesions tend to different regions of the figure but there i some significant overlap of points which would probably hard to deconvolve. Maybe we've lost too much of the original data's information and we should chose the number of PCs according to how much variance we want to preserve. We can specify that percentage instead the number of PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preserving 0.95 variance requires 3 PCs.\n"
     ]
    }
   ],
   "source": [
    "# do a PCA preserving 0.95 of the original data's variance\n",
    "var = 0.95\n",
    "pca = PCA(n_components=var)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "n_comp = len(pca.components_)\n",
    "print(\"Preserving {} variance requires {} PCs.\".format(var,n_comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preserving 0.99 variance requires 6 PCs.\n"
     ]
    }
   ],
   "source": [
    "# do a PCA preserving 0.99 of the original data's variance\n",
    "var = 0.99\n",
    "pca = PCA(n_components=var)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "n_comp = len(pca.components_)\n",
    "print(\"Preserving {} variance requires {} PCs.\".format(var,n_comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preserving 0.999 variance requires 11 PCs.\n"
     ]
    }
   ],
   "source": [
    "# do a PCA preserving 0.999 of the original data's variance\n",
    "var = 0.999\n",
    "pca = PCA(n_components=var)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "n_comp = len(pca.components_)\n",
    "print(\"Preserving {} variance requires {} PCs.\".format(var,n_comp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of preserved variance can be used as a hyper-parameter in a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation Grid search:\n",
      "Best parameter (CV F1 score=0.916):\n",
      "{'pca__n_components': 0.999, 'svc__C': 3162.2776601683795}\n",
      "\n",
      "Performance on the test set:\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 4  0]\n",
      " [ 2 10]]\n",
      "F1: 0.909\n",
      "Recall: 0.833\n",
      "Precision: 1.000\n",
      "Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "# put a standard-scaled PCA and a support vector calssifier in a pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "               (\"pca\",PCA(whiten=True)),\n",
    "               (\"svc\",SVC(kernel=\"linear\",class_weight=\"balanced\"))])\n",
    "\n",
    "# define paramter space to search durring grid search\n",
    "param_grid = {\"svc__C\":np.logspace(-4,6,5),\n",
    "              \"pca__n_components\":[0.95,0.99,0.999]}\n",
    "\n",
    "# perform grid search\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid,\n",
    "                    n_jobs=-1, cv=6, scoring=\"f1\",\n",
    "                    iid=False, verbose=0)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "print(\"Cross validation Grid search:\")\n",
    "print(\"Best parameter (CV F1 score=%0.3f):\" % grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "print(\"\\nPerformance on the test set:\\n\")\n",
    "print(\"Confusion matrix:\")\n",
    "y_pred = grid.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"F1: {:0.3f}\".format(f1_score(y_test, y_pred)))\n",
    "print(\"Recall: {:0.3f}\".format(recall_score(y_test, y_pred)))\n",
    "print(\"Precision: {:0.3f}\".format(precision_score(y_test, y_pred)))\n",
    "print(\"Accuracy: {:0.3f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation score was slightly improved, but in our test set there are still two instances misclassified. There is a version of kernel PCA (which basically employs the kernel trick like support vector machines) which allows it to perform nonlinear projections for dimensionality reduction. Let's see if we can improve our results with kernel PCA. Unfortunately we can not directly specify the amount of variance we want to preserve, so we'll have to calculate manually how many components will be needed for a certain desired preserved variance given a certain kernel (e.g. linear, rbf, polynomial, cosine, using default parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kPCA with linear kernel:\n",
      "Preserving 0.95 variance requires 3 PCs.\n",
      "Preserving 0.99 variance requires 6 PCs.\n",
      "Preserving 0.999 variance requires 11 PCs.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# set up a kPCA with linear kernel and calculate all PCs\n",
    "kpca = KernelPCA(kernel=\"linear\")\n",
    "X_train_kpca = kpca.fit_transform(X_train)\n",
    "# calculate the explained variance\n",
    "explained_variance = np.var(X_train_kpca, axis=0)\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "cumsum = np.cumsum(explained_variance_ratio)\n",
    "# print results\n",
    "print(\"For kPCA with linear kernel:\")\n",
    "for v in [0.95,0.99,0.999]:\n",
    "    for i,c in enumerate(cumsum):\n",
    "        if c>v:\n",
    "            print(\"Preserving {} variance requires {} PCs.\".format(v,i+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kPCA with rbf kernel:\n",
      "Preserving 0.95 variance requires 57 PCs.\n",
      "Preserving 0.99 variance requires 59 PCs.\n",
      "Preserving 0.999 variance requires 59 PCs.\n"
     ]
    }
   ],
   "source": [
    "# set up a kPCA with rbf kernel and calculate all PCs\n",
    "kpca = KernelPCA(kernel=\"rbf\")\n",
    "X_train_kpca = kpca.fit_transform(X_train)\n",
    "# calculate the explained variance\n",
    "explained_variance = np.var(X_train_kpca, axis=0)\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "cumsum = np.cumsum(explained_variance_ratio)\n",
    "# print results\n",
    "print(\"For kPCA with rbf kernel:\")\n",
    "for v in [0.95,0.99,0.999]:\n",
    "    for i,c in enumerate(cumsum):\n",
    "        if c>v:\n",
    "            print(\"Preserving {} variance requires {} PCs.\".format(v,i+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kPCA with polynomial kernel:\n",
      "Preserving 0.95 variance requires 1 PCs.\n",
      "Preserving 0.99 variance requires 2 PCs.\n",
      "Preserving 0.999 variance requires 4 PCs.\n"
     ]
    }
   ],
   "source": [
    "# set up a kPCA with polynomial kernel and calculate all PCs\n",
    "kpca = KernelPCA(kernel=\"poly\")\n",
    "X_train_kpca = kpca.fit_transform(X_train)\n",
    "# calculate the explained variance\n",
    "explained_variance = np.var(X_train_kpca, axis=0)\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "cumsum = np.cumsum(explained_variance_ratio)\n",
    "# print results\n",
    "print(\"For kPCA with polynomial kernel:\")\n",
    "for v in [0.95,0.99,0.999]:\n",
    "    for i,c in enumerate(cumsum):\n",
    "        if c>v:\n",
    "            print(\"Preserving {} variance requires {} PCs.\".format(v,i+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kPCA with cosine kernel:\n",
      "Preserving 0.95 variance requires 8 PCs.\n",
      "Preserving 0.99 variance requires 12 PCs.\n",
      "Preserving 0.999 variance requires 18 PCs.\n"
     ]
    }
   ],
   "source": [
    "# set up a kPCA with cosine kernel and calculate all PCs\n",
    "kpca = KernelPCA(kernel=\"cosine\")\n",
    "X_train_kpca = kpca.fit_transform(X_train)\n",
    "# calculate the explained variance\n",
    "explained_variance = np.var(X_train_kpca, axis=0)\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "cumsum = np.cumsum(explained_variance_ratio)\n",
    "# print results\n",
    "print(\"For kPCA with cosine kernel:\")\n",
    "for v in [0.95,0.99,0.999]:\n",
    "    for i,c in enumerate(cumsum):\n",
    "        if c>v:\n",
    "            print(\"Preserving {} variance requires {} PCs.\".format(v,i+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use both the kernel type and the preserved variance level in hyper-parameter tuning, we should write our own transformer class which handles that easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class kPCA(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"This class allows to tune kernel-type and variance-to-preserve as hyper-paramters\"\"\"\n",
    "    \n",
    "    pc_dict = {\"linear\":{0.95:3,0.99:6,0.999:11},\n",
    "               \"rbf\":{0.95:57,0.99:59,0.999:59},\n",
    "               \"poly\":{0.95:1,0.99:2,0.999:4},\n",
    "               \"cosine\":{0.95:8,0.99:12,0.999:18}}\n",
    "    \n",
    "    def __init__(self, kernel=\"linear\", var=0.95):\n",
    "        self.kernel = kernel\n",
    "        self.var = var\n",
    "        self.kpca = KernelPCA(kernel=self.kernel,\n",
    "                              n_components=self.pc_dict[kernel][var])\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.kpca.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return self.kpca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can up our performance in a final grid search with different kernel PCAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation Grid search:\n",
      "Best parameter (CV F1 score=0.870):\n",
      "{'kpca__kernel': 'linear', 'kpca__var': 0.95, 'svc__C': 100.0}\n",
      "\n",
      "Performance on the test set:\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 4  0]\n",
      " [ 2 10]]\n",
      "F1: 0.909\n",
      "Recall: 0.833\n",
      "Precision: 1.000\n",
      "Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "# put the custom kPCA, a standard scaler, and a support vector calssifier in a pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "               (\"kpca\",kPCA()),\n",
    "               (\"scaler\", StandardScaler()),\n",
    "               (\"svc\",SVC(kernel=\"linear\",class_weight=\"balanced\"))])\n",
    "\n",
    "# define paramter space to search durring grid search\n",
    "param_grid = {\"kpca__kernel\":[\"linear\",\"rbf\",\"poly\",\"cosine\"],\n",
    "              \"kpca__var\":[0.95,0.99,0.999],\n",
    "              \"svc__C\":np.logspace(-6,6,4)}\n",
    "\n",
    "# perform grid search\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid,\n",
    "                    n_jobs=-1, cv=6, scoring=\"f1\",\n",
    "                    iid=False, verbose=0)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "print(\"Cross validation Grid search:\")\n",
    "print(\"Best parameter (CV F1 score=%0.3f):\" % grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "print(\"\\nPerformance on the test set:\\n\")\n",
    "print(\"Confusion matrix:\")\n",
    "y_pred = grid.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"F1: {:0.3f}\".format(f1_score(y_test, y_pred)))\n",
    "print(\"Recall: {:0.3f}\".format(recall_score(y_test, y_pred)))\n",
    "print(\"Precision: {:0.3f}\".format(precision_score(y_test, y_pred)))\n",
    "print(\"Accuracy: {:0.3f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, looks like we weren't really able to achieve much of an improvement by dimensionality reduction but we also didn't make it worse. That means that the information that is in the data was as easy to grasp for the algorithm in higher dimensional space as in dimensionality reduced space. We do gain one big advantage through dimensionality reduction though: Significantly less computational cost! For this example with only 76 instances this may not seem relevant, but for much larger data sets it will be a massive speed-up if we can reduce the number of features from hundreds to only about a dozen without sacrificing too much classification accuracy (or what ever score we use).\n",
    "\n",
    "Thanks a lot for following along through this example!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
