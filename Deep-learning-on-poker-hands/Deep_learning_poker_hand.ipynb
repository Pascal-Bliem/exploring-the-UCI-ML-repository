{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized deep neural networks learning poker hands\n",
    "\n",
    "This data set contains examples of possible poker hands which can be used to let a machine learning algorithm learn the rules of the game. It was used in a [research paper](https://pdfs.semanticscholar.org/c068/ea7807367573f4b5f98c0681fca665e9ef74.pdf) in which the authors, R. Cattral, F. Oppacher, and D. Deugo, used evolutionary and symbolic machine learning methods to extract comprehensible and strong rules (the rules of poker in this case) from it. We will try to achieve this with an optimized neural network.\n",
    "\n",
    "The data set was contributed to the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Poker+Hand) by Robert Cattral and Franz Oppacher from the Department of Computer Science of Carleton University, which is where I got it from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame the Problem\n",
    "In this small project, we're not going all the way to building a full poker-bot that can play the entire game of poker against other agents. We just want to see if and how well an algorithm can understand the rules of how poker hands (the combination of cards that result in a certain score) are composed.\n",
    "\n",
    "The data we will consider contains one million instances of poker hands. Each poker hand in the data set is an example consisting of five cards which are drawn from a poker deck of 52 cards. Each card has two attributes, its suit (S, 1 to 4, representing Hearts, Spades, Diamonds, and Clubs) and its rank (C, 1 to 13, representing Ace, 2, 3, ... , Queen, and King). For each instance (poker hand) that results in 10 features. There are ten possible classes (0 to 9) which correspond to the card [combinations](https://en.wikipedia.org/wiki/Poker#Gameplay) that can be observed in the game of poker: nothing in hand, one pair, two pairs, three of a kind, straight, flush, full house, four of a kind, straight flush, and royal flush.\n",
    "\n",
    "We will treat this as a supervised multi-class classification problem. The rules of this classification are fairly complicated but we also have a lot of instances to train on, which makes the problem very suitable for deep neural networks. We will work with a multilayer perceptron architecture, implement it with `TensorFlow 2.0`'s `Keras` API, and optimize its hyper-parameters with the `hyperas` library using a bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries we'll need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "Let's import the data into data frame hand have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "      <th>S4</th>\n",
       "      <th>S5</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>808385</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234800</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196245</th>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609543</th>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961547</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325426</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752949</th>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958119</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116907</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439993</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        C1  C2  C3  C4  C5  S1  S2  S3  S4  S5  target\n",
       "808385   3   8   6  12   3   4   1   3   4   3       1\n",
       "234800   9  12   5  10   5   3   2   3   3   2       1\n",
       "196245  12   6   7   3   9   4   3   2   2   2       0\n",
       "609543  12  11  13   6   8   2   3   2   3   3       0\n",
       "961547  11   3  11  10   6   3   2   4   4   4       1\n",
       "325426   9  12   4   4   6   2   2   3   2   1       1\n",
       "752949  13   7   4   6   2   4   4   4   1   2       0\n",
       "958119   1   6   6  13  10   4   1   4   4   4       1\n",
       "116907   1  10   8   4  11   3   4   1   2   1       0\n",
       "439993   2   9   4   8   2   1   3   2   2   4       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data from CSV file\n",
    "df = pd.read_csv(\"poker-hand-testing.data\",\n",
    "                 names = [\"S1\",\"C1\",\"S2\",\"C2\",\"S3\",\"C3\",\n",
    "                          \"S4\",\"C4\",\"S5\",\"C5\",\"target\"])\n",
    "\n",
    "# order the columns to have the ranks (C, numerical) \n",
    "# and suits (S, categorical) together\n",
    "df = df[[\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\n",
    "         \"S1\",\"S2\",\"S3\",\"S4\",\"S5\",\n",
    "         \"target\"]]\n",
    "\n",
    "# let's have a look by taking a random sample\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 11 columns):\n",
      "C1        1000000 non-null int64\n",
      "C2        1000000 non-null int64\n",
      "C3        1000000 non-null int64\n",
      "C4        1000000 non-null int64\n",
      "C5        1000000 non-null int64\n",
      "S1        1000000 non-null int64\n",
      "S2        1000000 non-null int64\n",
      "S3        1000000 non-null int64\n",
      "S4        1000000 non-null int64\n",
      "S5        1000000 non-null int64\n",
      "target    1000000 non-null int64\n",
      "dtypes: int64(11)\n",
      "memory usage: 83.9 MB\n"
     ]
    }
   ],
   "source": [
    "# print the data frame's info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1        0\n",
       "C2        0\n",
       "C3        0\n",
       "C4        0\n",
       "C5        0\n",
       "S1        0\n",
       "S2        0\n",
       "S3        0\n",
       "S4        0\n",
       "S5        0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look for missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are apparently no missing values. Let's look at how the target classes are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    501209\n",
       "1    422498\n",
       "2     47622\n",
       "3     21121\n",
       "4      3885\n",
       "5      1996\n",
       "6      1424\n",
       "7       230\n",
       "8        12\n",
       "9         3\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we're dealing with a very strong class imbalance. Some of the poker hands are much rarer than others. We can try to account for this by handing class weights to the classifier but it will certainly be very difficult, if not impossible, to correctly classify e.g. a straight flush or royal flush.\n",
    "\n",
    "Let's also have a look at how the features are distributed to make sure there are no outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot feature distribution as histograms\n",
    "_ = df.hist(bins=13,figsize=(6,5))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, looks like the data is actually distributed as specified. No need to account for invalid values. We can now proceed to splitting of test and validation data sets. Because of the class imbalance, we will stratify the split so that approximately equal class distributions will be present in all sets. For the very rare classes this may not work perfectly. We'll also compute the class weights already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# split off a test set and stratify for classes (target)\n",
    "train_full, test = train_test_split(df,\n",
    "                                    stratify = df[\"target\"],\n",
    "                                    test_size=0.2, \n",
    "                                    random_state=123)\n",
    "\n",
    "# split into a training and a validation set and stratify for classes (target)\n",
    "train, valid = train_test_split(train_full,\n",
    "                                stratify = train_full[\"target\"],\n",
    "                                test_size=0.2,\n",
    "                                random_state=123)\n",
    "\n",
    "# compute class weigths which will be used to account for class imbalance\n",
    "class_weights = compute_class_weight(\"balanced\",\n",
    "                                     np.unique(train[\"target\"]),\n",
    "                                     train[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I really like using handy `pandas` and `sklearn` functions for exploring and preprocessing moderate-sized data. But for really big data that has to be distributed over several machines, we might prefer to use `dask` data frames or to do everything just in `TensorFlow`. `TensorFlow`'s data API can read data from different sources, handle preprocessing (even though I personally don't always find it very handy), and provide tensor datasets and `TensorFlow` also offers different kinds of feature columns which can be directly fed into a model. We'll use these features here to explore them a bit, even though we could also just use `sklearn`'s preprocessing on `pandas`'s data frames here on my single poor old laptop.\n",
    "\n",
    "We'll first define a function that turns the `pandas` data frames into `TensorFlow` data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    \"\"\"\n",
    "    A function that turns pandas data frames into TensorFlow data sets.\n",
    "    Params:\n",
    "    ----------\n",
    "    dataframe: input pandas data frame\n",
    "    shuffle: if True, the data set will be pre-shuffled\n",
    "    batch_size: batch size, meaning number of instances that \n",
    "                will be passed to model per optimization step\n",
    "    Returns:\n",
    "    ---------\n",
    "    ds: the output data set\n",
    "    \"\"\"\n",
    "    # get copy of data frame\n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    # extract class labels\n",
    "    labels = dataframe[\"target\"]\n",
    "    \n",
    "    # make data set from features (first 10 columns) and labels\n",
    "    # note: it's important to convert data type from int to float32\n",
    "    # here, TF doesn't do that automatically and will throw error\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe.iloc[:,:10]\n",
    "                                                  .astype(np.float32)), \n",
    "                                             labels))\n",
    "    # shuffle if desired\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=10000)\n",
    "    \n",
    "    # enable infinite repeat with given batch size and prefetch\n",
    "    ds = ds.repeat().batch(batch_size).prefetch(1)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually transform the data frames into `TensofFlow` data sets which will be fed to our model. But we'll have to choose the batch size first, how many instances will be passed through the model per optimization step.\n",
    "\n",
    "The batch size should be large enough to give a precise enough estimate of gradients during optimization but not so large that it significantly slows down the training iterations. In practice 32 is often chosen as default. At least that's what I understood from `TensofFlow` tutorials and a couple of deep learning books. I have no reason not to trust this advice here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# create tensorflow data sets from data frames\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "valid_ds = df_to_dataset(valid, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to normalize the numerical data (first 5 columns) before we send it into the model. We can provide a normalization function to the numerical feature column later but we'll first have to compute each feature's mean and standard deviation. We can compute them as `TensorFlow` tensors as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate means and standard deviations for the numerical features\n",
    "X_means = tf.math.reduce_mean(train.iloc[:,:5].values.astype(np.float32),axis=0)\n",
    "X_stds = tf.math.reduce_std(train.iloc[:,:5].values.astype(np.float32),axis=0)\n",
    "# since they all have approximately the same distribution, the means should all\n",
    "# be around 7.0 and the standard deviations around 3.74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the feature columns. We will have 5 numerical columns corresponding to the cards' ranks, which we will normalize. There will also be 5 categorical columns for the suits. Since they are ordinal and represented as integers here, we can first put them in 5 numerical columns and then transform these into 5 bucketized columns with 4 buckets each, one for each suit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all feature columns in a list which we'll\n",
    "# later feed to the model's input layer\n",
    "feature_columns = []\n",
    "\n",
    "# set up the numerical columns for the cards' ranks\n",
    "# with normalization function: (x-mean)/std\n",
    "for i, header in enumerate([\"C1\", \"C2\", \"C3\", \"C4\", \"C5\"]):\n",
    "    # set up numerical column\n",
    "    num_col = tf.feature_column.numeric_column(header,\n",
    "                                               normalizer_fn=lambda X:\n",
    "                                               (X - X_means[i]) / X_stds[i])\n",
    "    # append column to list\n",
    "    feature_columns.append(num_col)\n",
    "\n",
    "# set up the bucket columns for the cards' suits\n",
    "for header in [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]:\n",
    "\n",
    "    # set up bucket column\n",
    "    num_col = tf.feature_column.numeric_column(header)\n",
    "    bucket_col = tf.feature_column.bucketized_column(num_col,\n",
    "                                                     boundaries=list(\n",
    "                                                         range(2, 5)))\n",
    "    # append column to list\n",
    "    feature_columns.append(bucket_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "Now let's set up an initial model with Keras' seqential API, without thinking too much about parameters yet. We'll use 2 hidden layers, 200 units per layer, with exponential linear unit (elu) activation functions and He-initialized weights. After each hidden layer, we'll apply batch normalization to prevent vanishing or exploding gradients during training. We can also implement a early-stopping callback which will act against overfitting, as it just aborts training when the validation loss begins to increase again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 20000 steps, validate for 5000 steps\n",
      "Epoch 1/100\n",
      "19994/20000 [============================>.] - ETA: 0s - loss: 0.8818 - acc: 0.5876\n",
      "Epoch 00001: val_loss improved from inf to 0.80858, saving model to training/poker_hand_model.ckpt\n",
      "20000/20000 [==============================] - 130s 7ms/step - loss: 0.8817 - acc: 0.5877 - val_loss: 0.8086 - val_acc: 0.6579\n",
      "Epoch 2/100\n",
      "19998/20000 [============================>.] - ETA: 0s - loss: 0.6582 - acc: 0.7131\n",
      "Epoch 00002: val_loss did not improve from 0.80858\n",
      "20000/20000 [==============================] - 115s 6ms/step - loss: 0.6582 - acc: 0.7131 - val_loss: 1.1534 - val_acc: 0.6660\n",
      "Epoch 3/100\n",
      "19999/20000 [============================>.] - ETA: 0s - loss: 0.3995 - acc: 0.8401\n",
      "Epoch 00003: val_loss improved from 0.80858 to 0.77816, saving model to training/poker_hand_model.ckpt\n",
      "20000/20000 [==============================] - 121s 6ms/step - loss: 0.3995 - acc: 0.8401 - val_loss: 0.7782 - val_acc: 0.8856\n",
      "Epoch 4/100\n",
      "19994/20000 [============================>.] - ETA: 0s - loss: 0.3132 - acc: 0.8805\n",
      "Epoch 00004: val_loss improved from 0.77816 to 0.31105, saving model to training/poker_hand_model.ckpt\n",
      "20000/20000 [==============================] - 130s 7ms/step - loss: 0.3132 - acc: 0.8805 - val_loss: 0.3110 - val_acc: 0.9051\n",
      "Epoch 5/100\n",
      "19998/20000 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.8996\n",
      "Epoch 00005: val_loss did not improve from 0.31105\n",
      "20000/20000 [==============================] - 118s 6ms/step - loss: 0.2695 - acc: 0.8996 - val_loss: 1.1440 - val_acc: 0.7433\n",
      "Epoch 6/100\n",
      "19989/20000 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9187\n",
      "Epoch 00006: val_loss did not improve from 0.31105\n",
      "20000/20000 [==============================] - 123s 6ms/step - loss: 0.2235 - acc: 0.9187 - val_loss: 8.0383 - val_acc: 0.8846\n",
      "Epoch 7/100\n",
      "19993/20000 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9282Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.31105\n",
      "20000/20000 [==============================] - 120s 6ms/step - loss: 0.2006 - acc: 0.9282 - val_loss: 5.8794 - val_acc: 0.9416\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "# bulding the sequential model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.DenseFeatures(feature_columns),\n",
    "    keras.layers.Dense(200, activation='elu',kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(200, activation='elu',kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# model checkpoint call back to save checkpints during training\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"training/poker_hand_model.ckpt\",\n",
    "                                                save_best_only=True,\n",
    "                                                save_weights_only=True,\n",
    "                                                verbose=1)\n",
    "\n",
    "# early stopping callback to prevent overfitting on the  training data\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(patience=3,\n",
    "                                             min_delta=0.01,\n",
    "                                             restore_best_weights=True,\n",
    "                                             verbose=1)\n",
    "\n",
    "\n",
    "# Compile the model with a Nadam optimizer, initial learning rate \n",
    "# of 0.01 is relatively high.\n",
    "# Since classes are given in one vector as integers from 0 to 9,\n",
    "# we have to use sparse_categorical_crossentropy instead of\n",
    "# categorical_crossentropy as the loss function.\n",
    "model.compile(optimizer=keras.optimizers.Nadam(learning_rate=0.01),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"acc\"])\n",
    "\n",
    "# fit the model with trianing and validation data, considering class weights \n",
    "# and early stopping (hence 100 epochs probably wont run)\n",
    "history = model.fit(train_ds,\n",
    "          steps_per_epoch=len(train) // batch_size,\n",
    "          validation_data=valid_ds,\n",
    "          validation_steps=len(valid) // batch_size,\n",
    "          class_weight=class_weights,\n",
    "          epochs=100,\n",
    "          callbacks=[earlystop_cb,checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the training took a long time we better want to save (or load) the model in a serialized form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model (in TensorFlow's serialized SavedModel format, \n",
    "# you can also save it in HDF5 format by adding the file ending .h5)\n",
    "model.save(\"saved_model/poker_hand_keras_model\")\n",
    "\n",
    "# load the model again\n",
    "model = keras.models.load_model(\"saved_model/poker_hand_keras_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==============================] - 16s 3ms/step - loss: 0.2396 - acc: 0.9068\n",
      "Accuracy:  0.906845 \n",
      "Loss:  0.23963412045776844\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test set\n",
    "loss, accuracy = model.evaluate(test_ds,steps=len(test)//batch_size)\n",
    "print(\"Accuracy: \", accuracy, \"\\nLoss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, 95% accuracy, not bad for the first shot. Let's make predictions for the first 10 instances of our test set. We have a look at the targets first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 1, 1, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[0:10,:].target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first 10 instances from the test data and convert them to data set format\n",
    "new_data = df_to_dataset(test.iloc[0:10,:],shuffle=False,batch_size=10)\n",
    "\n",
    "# predict class probability\n",
    "pred_proba = model.predict(new_data,steps=1) \n",
    "\n",
    "# predict classes\n",
    "pred_calsses = np.argmax(pred_proba,axis=1)\n",
    "\n",
    "pred_calsses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a look at the probabilities predicted for the classes, we can see that the model is actually not very confident in it's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.751, 0.248, 0.001, 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(pred_proba[0],decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice if we could be more certain for the minority classes as well. Accuracy is generally not a very god metric for problems with a high class imbalance. The macro-averaged F1 score may be a better metric to optimize for if we want to achieve a good classification for all classes. Furthermore, we've just blindly guessed how many layers and units per layer and so on we use in the model. We can probably do even better by optimizing the hyper-parameters. That's what we'll do in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hyper-parameter optimization we will use the `hyperas` library, which is an easy-to-use Keras wrapper of the `hyperopt` library. To perform the optimization, we will first have to define functions which provide the training and validation data and build the model. I didn't manage to get the data function take global variables, `hyperas` would keep throwing errors; hence, I provided the entire data preprocessing again in this data function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, loguniform\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "\n",
    "# function that provides the data for the optimization\n",
    "def data():\n",
    "    \"\"\"Data providing function\"\"\"\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # import data from CSV file\n",
    "    df = pd.read_csv(\"poker-hand-testing.data\",\n",
    "                     names = [\"S1\",\"C1\",\"S2\",\"C2\",\"S3\",\"C3\",\n",
    "                              \"S4\",\"C4\",\"S5\",\"C5\",\"target\"])\n",
    "    \n",
    "    # order the columns to have the ranks (C, numerical) \n",
    "    # and suits (S, categorical) together\n",
    "    df = df[[\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\n",
    "             \"S1\",\"S2\",\"S3\",\"S4\",\"S5\",\n",
    "             \"target\"]]\n",
    "    \n",
    "    # one-hot encode the suits features\n",
    "    df = pd.get_dummies(df,columns=[\"S1\",\"S2\",\"S3\",\"S4\",\"S5\"])\n",
    "\n",
    "    \n",
    "    # split off a test set and stratify for classes (target)\n",
    "    train_full, test = train_test_split(df,\n",
    "                                        stratify = df[\"target\"],\n",
    "                                        test_size=0.2, \n",
    "                                        random_state=123)\n",
    "    \n",
    "    # split into a training and a validation set and stratify for classes (target)\n",
    "    train, valid = train_test_split(train_full,\n",
    "                                    stratify = train_full[\"target\"],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=123)\n",
    "    \n",
    "    # compute class weigths which will be used to account for class imbalance\n",
    "    class_weights = compute_class_weight(\"balanced\",\n",
    "                                         np.unique(train[\"target\"]),\n",
    "                                         train[\"target\"])\n",
    "    \n",
    "    # split features and labels\n",
    "    X_train, y_train = train.drop(\"target\",axis=1), train[\"target\"].values\n",
    "    X_valid, y_valid = valid.drop(\"target\",axis=1), valid[\"target\"].values\n",
    "    X_test, y_test = test.drop(\"target\",axis=1), test[\"target\"].values\n",
    "    \n",
    "    # get rank and suit column names\n",
    "    suit_cols = X_train.columns.drop([\"C1\",\"C2\",\"C3\",\"C4\",\"C5\"])\n",
    "    rank_cols = [\"C1\",\"C2\",\"C3\",\"C4\",\"C5\"]\n",
    "    \n",
    "    # set up the preprocessor wit a scaler for the ranks\n",
    "    preprocess = ColumnTransformer(transformers=[\n",
    "        (\"std\",StandardScaler(),rank_cols),\n",
    "        (\"pass\",\"passthrough\",suit_cols)])\n",
    "    \n",
    "    # scale the rank values with a standard scaler\n",
    "    X_train = preprocess.fit_transform(X_train)\n",
    "    X_valid = preprocess.transform(X_valid)\n",
    "    X_test = preprocess.transform(X_test)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return X_train, y_train, X_vaild, y_vaild, X_test, y_test, class_weights\n",
    "    \n",
    "# function that builds the model for the optimization\n",
    "def build_model(X_train, y_train, X_vaild, y_vaild, X_test, y_test, class_weights):\n",
    "    \"\"\"Model providing function\"\"\"    \n",
    "    \n",
    "    # parameters to optimize\n",
    "    num_layers = {{choice([2,3,4])}}\n",
    "    num_units = int({{uniform(50,400)}})\n",
    "    learning_rate = {{loguniform(-8,-4)}}\n",
    "    batch_size=32\n",
    "    \n",
    "    print(f\"New parameters:\\nNumber of layers: {num_layers}\\nNumber of units: {num_units}\\nLearning rate: {learning_rate}\")\n",
    "    \n",
    "    # create model and add input layer with feature columns\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=X_train.shape[1:],batch_size=batch_size))\n",
    "    \n",
    "    # add hidden layer\n",
    "    for i in range(num_layers):\n",
    "        model.add(keras.layers.Dense(units=num_units,\n",
    "                                     activation='elu',\n",
    "                                     kernel_initializer=\"he_normal\"))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    # add ourput layer\n",
    "    model.add(keras.layers.Dense(units=10, \n",
    "                                 activation='softmax',))\n",
    "    \n",
    "    # compile model with optimizer\n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=learning_rate),\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"acc\"])\n",
    "    \n",
    "    # fitthe model with class weights applied\n",
    "    model.fit(x=X_train,\n",
    "              y=y_train,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(X_valid,y_valid),\n",
    "              class_weight=class_weights,\n",
    "              epochs=1,\n",
    "              verbose=0)\n",
    "    \n",
    "    # make class predictions with validation data\n",
    "    pred_proba = model.predict(X_valid) \n",
    "    pred_calsses = np.argmax(pred_proba,axis=1)\n",
    "    \n",
    "    # calculate the macro-averaged F1 score based on the predictions\n",
    "    f1 = f1_score(y_valid,pred_calsses,average=\"macro\")\n",
    "    \n",
    "    # print results of the optimization round\n",
    "    print(f\"F1 score: {f1:.3f}\\nfor\\nNumber of layers: {num_layers}\\nNumber of units: {num_units}\\nLearning rate: {learning_rate}\")\n",
    "    \n",
    "    \n",
    "    return {'loss': -f1, 'status': STATUS_OK, 'model': model}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the functions are set up, let's perform 30 optimization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a hyperas optimization\n",
    "best_run, best_model = optim.minimize(model=build_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=30,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='Deep_learning_poker_hand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the best parameters we found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we should train a model with 3 hidden layers, 265 units per layer, and a learning rate of 0.12. Let's do that and see how good we can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 20000 steps, validate for 5000 steps\n",
      "Epoch 1/100\n",
      "19996/20000 [============================>.] - ETA: 0s - loss: 0.8743 - acc: 0.5968\n",
      "Epoch 00001: val_loss improved from inf to 0.68111, saving model to training/poker_hand_model.ckpt\n",
      "20000/20000 [==============================] - 189s 9ms/step - loss: 0.8742 - acc: 0.5969 - val_loss: 0.6811 - val_acc: 0.7055\n",
      "Epoch 2/100\n",
      "19998/20000 [============================>.] - ETA: 0s - loss: 0.4881 - acc: 0.8041\n",
      "Epoch 00002: val_loss did not improve from 0.68111\n",
      "20000/20000 [==============================] - 178s 9ms/step - loss: 0.4881 - acc: 0.8041 - val_loss: 1.3419 - val_acc: 0.6085\n",
      "Epoch 3/100\n",
      "19996/20000 [============================>.] - ETA: 0s - loss: 0.3234 - acc: 0.8769\n",
      "Epoch 00003: val_loss did not improve from 0.68111\n",
      "20000/20000 [==============================] - 191s 10ms/step - loss: 0.3234 - acc: 0.8769 - val_loss: 98.4170 - val_acc: 0.8010\n",
      "Epoch 4/100\n",
      "19993/20000 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.9019\n",
      "Epoch 00004: val_loss did not improve from 0.68111\n",
      "20000/20000 [==============================] - 182s 9ms/step - loss: 0.2656 - acc: 0.9019 - val_loss: 1.0344 - val_acc: 0.6891\n",
      "Epoch 5/100\n",
      "19994/20000 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9150\n",
      "Epoch 00005: val_loss did not improve from 0.68111\n",
      "20000/20000 [==============================] - 182s 9ms/step - loss: 0.2323 - acc: 0.9150 - val_loss: 0.8909 - val_acc: 0.8548\n",
      "Epoch 6/100\n",
      "19996/20000 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9251Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.68111\n",
      "20000/20000 [==============================] - 186s 9ms/step - loss: 0.2076 - acc: 0.9251 - val_loss: 1609.0826 - val_acc: 0.8910\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "def build_opt_model():\n",
    "    # bulding the sequential model\n",
    "    opt_model = keras.Sequential([\n",
    "        keras.layers.DenseFeatures(feature_columns),\n",
    "        keras.layers.Dense(265, activation='elu', kernel_initializer=\"he_normal\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(265, activation='elu', kernel_initializer=\"he_normal\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(265, activation='elu', kernel_initializer=\"he_normal\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # model checkpoint call back to save checkpints during training\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "        \"training/poker_hand_model.ckpt\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=1)\n",
    "    \n",
    "    # early stopping callback to prevent overfitting on the  training data\n",
    "    earlystop_cb = keras.callbacks.EarlyStopping(patience=5,\n",
    "                                                 min_delta=0.01,\n",
    "                                                 restore_best_weights=True,\n",
    "                                                    verbose=1)\n",
    "    \n",
    "    # Compile the model with a Nadam optimizer    \n",
    "    opt_model.compile(optimizer=keras.optimizers.Nadam(learning_rate=0.012),\n",
    "                      loss=\"sparse_categorical_crossentropy\",\n",
    "                      metrics=[\"acc\"])\n",
    "    \n",
    "    return opt_model, checkpoint_cb, earlystop_cb\n",
    "    \n",
    "opt_model, checkpoint_cb, earlystop_cb = build_opt_model()\n",
    "\n",
    "# fit the model with trianing and validation data, considering class weights\n",
    "# and early stopping (hence 100 epochs probably wont run)\n",
    "history = opt_model.fit(train_ds,\n",
    "                        steps_per_epoch=len(train) // batch_size,\n",
    "                        validation_data=valid_ds,\n",
    "                        validation_steps=len(valid) // batch_size,\n",
    "                        class_weight=class_weights,\n",
    "                        epochs=100,\n",
    "                        callbacks=[earlystop_cb, checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model again see how we perform on the F1 score now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pascal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: saved_model/poker_hand_keras_opt_model/assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "opt_model.save(\"saved_model/poker_hand_keras_opt_model\")\n",
    "\n",
    "# load the model again\n",
    "opt_model = keras.models.load_model(\"saved_model/poker_hand_keras_opt_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro: 0.19\n",
      "F1-micro: 0.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# make predictions for the test set (steps = number of instances / batch size)\n",
    "pred_proba = opt_model.predict(test_ds,steps=6250) \n",
    "pred_calsses = np.argmax(pred_proba,axis=1)\n",
    "    \n",
    "# calculate the macro-averaged F1 score based on the predictions\n",
    "f1_macro = f1_score(test[\"target\"].values,pred_calsses,average=\"macro\")\n",
    "f1_micro = f1_score(test[\"target\"].values,pred_calsses,average=\"micro\")\n",
    "\n",
    "print(f\"F1-macro: {f1_macro:.2f}\\nF1-micro: {f1_micro:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the micro-averaged F1 score, which considers all instances individually, is not too bad. But the macro-averaged F1 score, which is an unweighted average of every classes' F1 score, is much worse. This clearly shows that we still have a big problem with classifying the strongly underrepresented classes correctly. This becomes clear when we look at the confusion matrix below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[95597,  4644,     1,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [40088, 44265,    53,    94,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [  538,  8284,    63,   639,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [   10,  2696,    60,  1458,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [  427,   350,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [  393,     6,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [    0,    74,     2,   209,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [    0,     0,     0,    42,     0,     0,     3,     1,     0,\n",
       "            0],\n",
       "       [    2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [    1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test[\"target\"].values,pred_calsses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling the data set to counter class imbalance\n",
    "As we have seen above, the neural network has trouble classifying the underrepresented classes correctly. One approach we can try is to bootstrap (sample with replacement) a new dataset from the original data set in which the minority classes are oversampled so that all classes are balanced. We don't actually provide any new data but since training a neural network is a stochastic process, we can try to reduce its bias towards the majority class with this method.\n",
    "\n",
    "Let's bootstrap 100000 samples for each of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame with 1,000,000 bootstrapped instances, 100,000 from each class\n",
    "df_resampled = pd.concat([\n",
    "    df.loc[df.target == label, :].sample(100000, replace=True)\n",
    "    for label in df.target.unique()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And split it into train, validation, and test sets again. The suffix _rs stands for resampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split off a test set and stratify for classes (target)\n",
    "train_full_rs, test_rs = train_test_split(df_resampled,\n",
    "                                          stratify=df_resampled[\"target\"],\n",
    "                                          test_size=0.2,\n",
    "                                          random_state=123)\n",
    "\n",
    "# split into a training and a validation set and stratify for classes (target)\n",
    "train_rs, valid_rs = train_test_split(train_full_rs,\n",
    "                                      stratify=train_full_rs[\"target\"],\n",
    "                                      test_size=0.2,\n",
    "                                      random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recompute column means and standard deviations (due to the oversampling of the minority classes, these quantities are quite a bit different now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([7.245467  7.151083  7.5978374 7.1570797 7.494875 ], shape=(5,), dtype=float32) tf.Tensor([3.775464  3.740848  3.7774026 3.7822576 3.6998467], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# calculate means and standard deviations for the numerical features\n",
    "X_means = tf.math.reduce_mean(train_rs.iloc[:, :5].values.astype(np.float32),\n",
    "                              axis=0)\n",
    "X_stds = tf.math.reduce_std(train_rs.iloc[:, :5].values.astype(np.float32),\n",
    "                            axis=0)\n",
    "print(X_means, X_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data to `TensorFlow` data set format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# create tensorflow data sets from data frames\n",
    "train_rs_ds = df_to_dataset(train_rs, batch_size=batch_size)\n",
    "valid_rs_ds = df_to_dataset(valid_rs, shuffle=False, batch_size=batch_size)\n",
    "test_rs_ds = df_to_dataset(test_rs, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a new model with the resampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 20000 steps, validate for 5000 steps\n",
      "Epoch 1/100\n",
      "19993/20000 [============================>.] - ETA: 0s - loss: 0.7889 - acc: 0.6875\n",
      "Epoch 00001: val_loss improved from inf to 0.38209, saving model to training/poker_hand_model.ckpt\n",
      "20000/20000 [==============================] - 213s 11ms/step - loss: 0.7888 - acc: 0.6876 - val_loss: 0.3821 - val_acc: 0.8474\n",
      "Epoch 2/100\n",
      "19998/20000 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.8611\n",
      "Epoch 00002: val_loss did not improve from 0.38209\n",
      "20000/20000 [==============================] - 149s 7ms/step - loss: 0.3663 - acc: 0.8611 - val_loss: 0.9238 - val_acc: 0.9216\n",
      "Epoch 3/100\n",
      "19993/20000 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9277\n",
      "Epoch 00003: val_loss did not improve from 0.38209\n",
      "20000/20000 [==============================] - 142s 7ms/step - loss: 0.2054 - acc: 0.9278 - val_loss: 184.8528 - val_acc: 0.9716\n",
      "Epoch 4/100\n",
      "19992/20000 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.9538\n",
      "Epoch 00004: val_loss did not improve from 0.38209\n",
      "20000/20000 [==============================] - 153s 8ms/step - loss: 0.1384 - acc: 0.9538 - val_loss: 8.2383 - val_acc: 0.9768\n",
      "Epoch 5/100\n",
      "19999/20000 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9666\n",
      "Epoch 00005: val_loss did not improve from 0.38209\n",
      "20000/20000 [==============================] - 182s 9ms/step - loss: 0.1036 - acc: 0.9666 - val_loss: 15.8309 - val_acc: 0.9879\n",
      "Epoch 6/100\n",
      "19999/20000 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9723Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.38209\n",
      "20000/20000 [==============================] - 175s 9ms/step - loss: 0.0877 - acc: 0.9723 - val_loss: 433.6815 - val_acc: 0.9931\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "opt_model_rs, checkpoint_cb, earlystop_cb = build_opt_model()\n",
    "\n",
    "# fit the model with trianing and validation data, considering class weights\n",
    "# and early stopping (hence 100 epochs probably wont run)\n",
    "history_rs = opt_model_rs.fit(train_rs_ds,\n",
    "                              steps_per_epoch=len(train_rs) // batch_size,\n",
    "                              validation_data=valid_rs_ds,\n",
    "                              validation_steps=len(valid_rs) // batch_size,\n",
    "                              epochs=100,\n",
    "                              callbacks=[earlystop_cb, checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/poker_hand_keras_opt_model_rs/assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "opt_model_rs.save(\"saved_model/poker_hand_keras_opt_model_rs\")\n",
    "\n",
    "# load the model again\n",
    "opt_model_rs = keras.models.load_model(\"saved_model/poker_hand_keras_opt_model_rs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's make some predictions again and look at the F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro: 0.84\n",
      "F1-micro: 0.85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[13296,  4792,   971,     5,   911,    25,     0,     0,     0,\n",
       "            0],\n",
       "       [ 2294,  9522,  4722,  1308,  2064,     8,    82,     0,     0,\n",
       "            0],\n",
       "       [   14,  2171, 11278,  2226,  2128,     0,  2141,    42,     0,\n",
       "            0],\n",
       "       [    0,    34,   303, 15868,    28,     0,  2648,  1119,     0,\n",
       "            0],\n",
       "       [   10,    72,    29,     0, 19889,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [    0,     0,     0,     0,     0, 19943,     0,     0,    57,\n",
       "            0],\n",
       "       [    0,     0,     0,    36,     0,     0, 19784,   180,     0,\n",
       "            0],\n",
       "       [    0,     0,     0,     0,     0,     0,     0, 20000,     0,\n",
       "            0],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0, 20000,\n",
       "            0],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "        20000]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions for the test set (steps = number of instances / batch size)\n",
    "pred_proba = opt_model_rs.predict(test_rs_ds,steps=6250) \n",
    "pred_calsses = np.argmax(pred_proba,axis=1)\n",
    "    \n",
    "# calculate the macro-averaged F1 score based on the predictions\n",
    "f1_macro = f1_score(test_rs[\"target\"].values,pred_calsses,average=\"macro\")\n",
    "f1_micro = f1_score(test_rs[\"target\"].values,pred_calsses,average=\"micro\")\n",
    "\n",
    "print(f\"F1-macro: {f1_macro:.2f}\\nF1-micro: {f1_micro:.2f}\")\n",
    "confusion_matrix(test_rs[\"target\"].values,pred_calsses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the micro and macro averaged F1 score are almost identical. The resampling has certainly helped with the class imbalance and slightly improved the overall performance. Looking at the confusion matrix, we can see that the very strongly oversampled minority classes are unsurprisingly all classified correctly because they're always the same reoccurring examples, whereas the original majority class still shows several misclassifications, most likely because not all possible examples of these classes were sampled from the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
