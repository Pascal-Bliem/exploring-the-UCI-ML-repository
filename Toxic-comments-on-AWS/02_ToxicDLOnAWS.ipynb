{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuation - Classifying toxic comments with deep learning\n",
    " \n",
    "This notebook is a continuation of my [previous](./01_ToxicCommentsOnAWS.ipynb) attempt to classify comments on Wikipedia articles according to their verbal toxicity and getting to know AWS SageMaker. In the previous notebook I used Scikit-Learn implementations of shallow, non-sequetial ML algorithms and the Scikit-Learn-specific parts of the AWS SageMaker Python SDK. This time I'll use 1DConv and LSTM neural nets implemented in TensorFlow and the corrsponding parts of the SageMaker Python SDK to seperately build the model in this notebook, launch a training job, and then invoke an endpoint instance to host the trained model. To keep things a little simpler, I'll not consider specific types of verbal toxicity this time but only if a comment is toxic at all or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First we need to set up everthing to run on AWS, namely the S3 bucket and IAM role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Define IAM role\n",
    "region = boto3.Session().region_name    \n",
    "smclient = boto3.Session().client(\"sagemaker\")\n",
    "role = get_execution_role()\n",
    "\n",
    "# S3 bucket\n",
    "bucket = '<my-bucket-name>'\n",
    "prefix = 'sagemaker/toxic-comments'\n",
    "\n",
    "# get the zsame ipped training and test data from S3 as last time \n",
    "# (the zip already contains differently preprocessed versions to save some time) \n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3.Bucket(bucket).download_file(\"jigsaw-toxic-comment-classification-challenge.zip\",\n",
    "                                \"local-jigsaw-toxic-comment-classification-challenge.zip\")\n",
    "\n",
    "# unzip the data\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"local-jigsaw-toxic-comment-classification-challenge.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data processing and computation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "# deep learning and corresponding data preprocessing\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "We already explored the data in detail in the first notebook, so we'll just do the processing here. We will start with the same train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24991</th>\n",
       "      <td>42176424aa902f06</td>\n",
       "      <td>Policy says to block both participants in an e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6867</th>\n",
       "      <td>125536354304fcca</td>\n",
       "      <td>Go Fuck Yourself \\n\\nDeeside College is a moth...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17750</th>\n",
       "      <td>2edb9cb73ea97eca</td>\n",
       "      <td>fiddle away, with your</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156845</th>\n",
       "      <td>d4707bf4a06d1855</td>\n",
       "      <td>\"\\n\\n Oregon Ducks football \\n\\nHi Abdoozy, so...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88759</th>\n",
       "      <td>ed759c8c9bc94c7b</td>\n",
       "      <td>\"\\nWelcome!\\n\\nHello, , and welcome to Wikiped...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "24991   42176424aa902f06  Policy says to block both participants in an e...   \n",
       "6867    125536354304fcca  Go Fuck Yourself \\n\\nDeeside College is a moth...   \n",
       "17750   2edb9cb73ea97eca                             fiddle away, with your   \n",
       "156845  d4707bf4a06d1855  \"\\n\\n Oregon Ducks football \\n\\nHi Abdoozy, so...   \n",
       "88759   ed759c8c9bc94c7b  \"\\nWelcome!\\n\\nHello, , and welcome to Wikiped...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "24991       0             0        0       0       0              0  \n",
       "6867        1             0        1       0       1              0  \n",
       "17750       0             0        0       0       0              0  \n",
       "156845      0             0        0       0       0              0  \n",
       "88759       0             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load training data\n",
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>148224</th>\n",
       "      <td>f7b7ff872e3a094c</td>\n",
       "      <td>\" \\n :::Are you aware of any recent articles w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78628</th>\n",
       "      <td>8333fb10ce0bfc19</td>\n",
       "      <td>==Bot on please?== \\n Hi Alex, I don't know if...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113904</th>\n",
       "      <td>be27f4723912ac6b</td>\n",
       "      <td>\" \\n\\n == Cultural relativism == \\n\\n \"\"Psycho...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52926</th>\n",
       "      <td>57e3ad0c96651f30</td>\n",
       "      <td>\" \\n :::Well, seeing as I'm supporting, I don'...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93310</th>\n",
       "      <td>9ba32b4beb22808a</td>\n",
       "      <td>and that he realised Wikipedia was gonna be a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "148224  f7b7ff872e3a094c  \" \\n :::Are you aware of any recent articles w...   \n",
       "78628   8333fb10ce0bfc19  ==Bot on please?== \\n Hi Alex, I don't know if...   \n",
       "113904  be27f4723912ac6b  \" \\n\\n == Cultural relativism == \\n\\n \"\"Psycho...   \n",
       "52926   57e3ad0c96651f30  \" \\n :::Well, seeing as I'm supporting, I don'...   \n",
       "93310   9ba32b4beb22808a  and that he realised Wikipedia was gonna be a ...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "148224      0             0        0       0       0              0  \n",
       "78628      -1            -1       -1      -1      -1             -1  \n",
       "113904      0             0        0       0       0              0  \n",
       "52926      -1            -1       -1      -1      -1             -1  \n",
       "93310       0             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test data\n",
    "test_df = pd.read_csv(\"./data/test.csv\").merge(pd.read_csv(\"./data/test_labels.csv\"),left_on=\"id\", right_on=\"id\")\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40701</th>\n",
       "      <td>439e70520e9e584a</td>\n",
       "      <td>We already quote Barron prominently in the led...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102576</th>\n",
       "      <td>ab42619b5656aa26</td>\n",
       "      <td>dates only from Protestant Reformation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96791</th>\n",
       "      <td>a180be3aa1103c0d</td>\n",
       "      <td>: Don't repeat the same comment in several pla...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129573</th>\n",
       "      <td>d884b8c801c193e9</td>\n",
       "      <td>Is this article and the Type 98 20 mm AA Half-...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44944</th>\n",
       "      <td>4a89a5fda298a1f2</td>\n",
       "      <td>:::What??? Have you read it???? It was written...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "40701   439e70520e9e584a  We already quote Barron prominently in the led...   \n",
       "102576  ab42619b5656aa26             dates only from Protestant Reformation   \n",
       "96791   a180be3aa1103c0d  : Don't repeat the same comment in several pla...   \n",
       "129573  d884b8c801c193e9  Is this article and the Type 98 20 mm AA Half-...   \n",
       "44944   4a89a5fda298a1f2  :::What??? Have you read it???? It was written...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "40701       0             0        0       0       0              0  \n",
       "102576      0             0        0       0       0              0  \n",
       "96791       0             0        0       0       0              0  \n",
       "129573      0             0        0       0       0              0  \n",
       "44944       0             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the -1 labeled instance (will be converted to float so convert back to int)\n",
    "test_df = test_df.where(test_df!=-1).dropna()\n",
    "test_df[test_df.columns[2:]] = test_df[test_df.columns[2:]].astype(np.int)\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labels\n",
    "y_train = train_df.iloc[:,2:].reset_index(drop=True).copy()\n",
    "y_test = test_df.iloc[:,2:].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous part we already preprocessed the data by removing stop words, punctuation, and number. Let's re-use this data here (and remember to remove the rows that are now completly empty due to the processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processed training and test features\n",
    "X_train_stop = pd.read_csv(\"./data/X_train_stop.csv\",skip_blank_lines=False)\n",
    "X_test_stop = pd.read_csv(\"./data/X_test_stop.csv\",skip_blank_lines=False)\n",
    "\n",
    "# get index of empty rows\n",
    "idx_train_stop = X_train_stop[X_train_stop.comment_text.isnull()].index\n",
    "idx_test_stop = X_test_stop[X_test_stop.comment_text.isnull()].index\n",
    "\n",
    "# remove empty rows\n",
    "X_train_stop = X_train_stop.drop(idx_train_stop)\n",
    "X_test_stop = X_test_stop.drop(idx_test_stop)\n",
    "y_train_stop = y_train.drop(idx_train_stop)\n",
    "y_test_stop = y_test.drop(idx_test_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the problem a bit simpler now, we'll only consider if a comment is toxic or not, not in which way it is toxic. This changes our task from a multi label to a binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make binary labels - either toxic or not\n",
    "y_train_bin = np.zeros([len(y_train_stop)])\n",
    "y_test_bin = np.zeros([len(y_test_stop)])\n",
    "\n",
    "for y_stop, y_bin in zip([y_train_stop,y_test_stop],[y_train_bin,y_test_bin]):\n",
    "    for i, labels in enumerate(y_stop.values):\n",
    "        if np.sum(labels) > 0:\n",
    "            y_bin[i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point our data is still in text format. If we want to feed it into a neural net, starting with an embedding layer, we should bring it in a numerical format. Since the comments have different length, we have to pad them with zeros so that the sequences we feed into the network all have the same lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a vocabulary of only the 50000 most popular unique words\n",
    "# and a sequence length of 700 (will be padded with zeros to len=700)\n",
    "max_features = 50000\n",
    "maxlen = 700\n",
    "\n",
    "# tokenizing and padding the comment sequences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train_stop.comment_text)\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train_stop.comment_text)\n",
    "X_train_seq = sequence.pad_sequences(train_sequences, maxlen=maxlen)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test_stop.comment_text)\n",
    "X_test_seq = sequence.pad_sequences(test_sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything should be as we need it and we can upload the training data to S3 so that the training job which we will launch later can fetch it from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory for training data\n",
    "WORK_DIRECTORY = \"train_data\"\n",
    "if not os.path.isdir(WORK_DIRECTORY):\n",
    "    os.mkdir(WORK_DIRECTORY)\n",
    "\n",
    "# write the trainingdata (features and labels) to CSV    \n",
    "np.save(\"./\"+WORK_DIRECTORY+\"/X_train_seq.npy\",X_train_seq)\n",
    "np.save(\"./\"+WORK_DIRECTORY+\"/y_train_bin.npy\",y_train_bin)\n",
    "\n",
    "# upload the data to S3 to be accessed for training later\n",
    "train_input = sagemaker.Session().upload_data(WORK_DIRECTORY, key_prefix=\"{}/{}\".format(prefix, WORK_DIRECTORY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "We will write a training script that parses all potential hyperparameters (I've already set the default the way I want them), loads the training data from S3, builds a model with an embedding layer, a Conv1D layer with max pooling, and a LSTM layer, going into a dense layer with Sigmoid activation for binary classification, and then train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script.py\n",
    "# write this notebook cell as a script file\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "\n",
    "# the training function\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters \n",
    "    parser.add_argument('--max_features', type=int, default=50000)\n",
    "    parser.add_argument('--maxlen', type=int, default=700)\n",
    "    parser.add_argument('--embedding_size', type=int, default=128)\n",
    "    parser.add_argument('--kernel_size', type=int, default=5)\n",
    "    parser.add_argument('--filters', type=int, default=64)\n",
    "    parser.add_argument('--pool_size', type=int, default=64)\n",
    "    parser.add_argument('--lstm_output_size', type=int, default=70)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output_data_dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # set hyperparameters\n",
    "    # Embedding\n",
    "    max_features = args.max_features\n",
    "    maxlen = args.maxlen\n",
    "    embedding_size = args.embedding_size\n",
    "\n",
    "    # Convolution\n",
    "    kernel_size = args.kernel_size\n",
    "    filters = args.filters\n",
    "    pool_size = args.pool_size\n",
    "\n",
    "    # LSTM\n",
    "    lstm_output_size = args.lstm_output_size\n",
    "    \n",
    "    # Training\n",
    "    batch_size = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    \n",
    "    # Take the set of input files \n",
    "    input_files = [ os.path.join(args.train, file) for file in sorted(os.listdir(args.train)) ]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "        \n",
    "    # load the input data from the files\n",
    "    X_train_seq = np.load(input_files[0],allow_pickle=True)\n",
    "    y_train_bin = np.load(input_files[1],allow_pickle=True)\n",
    "    \n",
    "    # compute class weigths\n",
    "    class_weights = class_weight.compute_class_weight(\"balanced\",[0.0,1.0],y_train_bin)\n",
    "    \n",
    "    # split of a validation set from the training data\n",
    "    X_train_seq, X_valid_seq, y_train_bin, y_valid_bin = train_test_split(X_train_seq, \n",
    "                                                                          y_train_bin,\n",
    "                                                                          stratify=y_train_bin, \n",
    "                                                                          test_size=0.25, \n",
    "                                                                          random_state=42)\n",
    "    \n",
    "    # convert to tensorflow dataset format\n",
    "    train_ds = (tf.data.Dataset.from_tensor_slices((X_train_seq, y_train_bin.astype(np.int32)))\n",
    "                .repeat()\n",
    "                .shuffle(100)\n",
    "                .batch(batch_size)\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "    valid_ds = (tf.data.Dataset.from_tensor_slices((X_valid_seq, y_valid_bin.astype(np.int32)))\n",
    "                .repeat()\n",
    "                .batch(batch_size)\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    \n",
    "    # build the model - staring with and embedding layer,\n",
    "    # feeding into a 1DConv and an LSTM\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding=\"valid\",\n",
    "                     activation=\"relu\",\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(lstm_output_size))\n",
    "    model.add\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=0)\n",
    "    earlystop = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0.01, patience=0, restore_best_weights=True)\n",
    "    \n",
    "    # print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # start the training\n",
    "    print('Train...')\n",
    "    history = model.fit(train_ds,\n",
    "                        epochs=epochs,\n",
    "                        steps_per_epoch=len(X_train_seq)//batch_size,\n",
    "                        validation_data=valid_ds,\n",
    "                        validation_steps=len(X_valid_seq)//batch_size,\n",
    "                        class_weight=class_weights,\n",
    "                        callbacks=[lr_scheduler,earlystop])\n",
    "    \n",
    "    # save the model\n",
    "    \n",
    "    if not os.path.isdir(args.model_dir):\n",
    "        os.mkdir(args.model_dir)\n",
    "        \n",
    "    model.save(os.path.join(args.model_dir, \"model.h5\"))\n",
    "    \n",
    "    \n",
    "# the inference function\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Deserialized and return fitted model\n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    model = keras.models.load_model(os.path.join(model_dir, \"model.h5\"))\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now that we have specified what sould be done during training in the script file, we can easily create a Sagemaker estimator from its prebuild Tensorflow container. We'll just specify the script's location, our role, and on what type of instance we want to perform the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# build a TensorFLow estimator\n",
    "tf_estimator = TensorFlow(entry_point=\"script.py\", role=role,\n",
    "                          train_instance_count=1, train_instance_type=\"ml.m4.xlarge\",\n",
    "                          framework_version=\"2.0.0\", py_version=\"py3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the training with specifying the S3 location with the training data\n",
    "tf_estimator.fit({'train': train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "If we now want to use the trained model, we'll either first have to deploy it to an endpoint instance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor = tf_estimator.deploy(initial_instance_count=1,\n",
    "#                            instance_type=\"ml.m4.xlarge\",\n",
    "#                             endpoint_type=\"tensorflow-serving\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or we just load the trained model into this notebook instance here (e.g. AWS thinks I'm already above my deployment instance quota)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the trained model from the S3 bucket\n",
    "s3.Bucket(bucket).download_file(\"model.h5\",\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = keras.models.load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bring the test data in the right format and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "test_ds = (tf.data.Dataset.from_tensor_slices((X_test_seq, y_test_bin.astype(np.int32)))\n",
    "            .batch(batch_size)\n",
    "            .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for the test data\n",
    "y_pred = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.where(y_pred>=0.5,1.0,0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's see how the model performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 macro: 0.6635685459214871\n",
      "Accuracy: 0.9138630136986301\n",
      "Confusion matrix:\n",
      "[[52947  4686]\n",
      " [  816  5426]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "f1 = f1_score(y_test_bin, y_pred, average=\"binary\")\n",
    "acc = accuracy_score(y_test_bin, y_pred)\n",
    "cm = confusion_matrix(y_test_bin, y_pred, labels=[0,1])\n",
    "print(f\"F1 macro: {f1}\\nAccuracy: {acc}\\nConfusion matrix:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the confusion matrix that still a lot of instances are misclassified, but overall, this model results in a better F1 score and accuracy than the shallow multi-label model. This may be more due to the fact that a binary classification problem is easier and not so much due to emplyoing deep learning vs. shallow learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We used deep, sequential machine learning methods to classify Wikipedia article comments verbally toxic or not, using AWS Sagemakers capabilities of buliding and training models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
