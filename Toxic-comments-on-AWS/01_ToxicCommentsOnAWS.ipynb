{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Wikipedia comments according to their verbal toxicity\n",
    " \n",
    "## Introduction\n",
    "Personal attacks in the form of rude or threatening comments have a severely damaging effect on online discussions. Many people who feel attacked personally or repulsed by abusive tone may stop expressing their own views and give up on looking out for other people's opinions. Many hosts of news or media platforms struggle with handling such attacks and may be forced to limit or shut down commenting activities completely, thereby, extinguishing all discussion on the matter. In a society in which we want to reach better conclusions by exchange of opinion and discussion, such behaviour causes tremendous damage to the public discourse. Having this in mind, it would be highly desirable to have the technological means to block the posting of toxic comments while the rest of the reasonable discussion can take place undisturbed. Developing a model for classifying toxic comments is the aim of this project.\n",
    " \n",
    "A technical note: I'm running this project on AWS Sagemaker (actually I'm in the process of familiarizing myself with Sagemaker) and I'm also importing the data from my AWS S3 bucket. If you would like to run the code in this notebook yourself on AWS, you can download the [data](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data) and adjust the file path, credentials, etc.\n",
    " \n",
    "## Framing the problem\n",
    "The data set we will be using consists of toxic and non-toxic comments and is made for a text classification task. It originates from a [Kaggle competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) which was hosted in 2017. It contains comments from Wikipedia which have to be classified as either non-toxic or into six categories of toxicity: `toxic`, `severe_toxic`, `obscene`, `threat`, `insult`, and `identity_hate`. The labeling was done via crowdsourcing which means that the dataset was labeled subjectively by different people who may not always agree 100% on the choice of labels. This may introduce some level of inconsistency and hence, inaccuracy. More information on this data set can be found in this [paper](https://arxiv.org/pdf/1610.08914.pdf). \n",
    " \n",
    "To achieve the most accurate classification we will employ a variety of different preprocessing and machine learning classification methods and see which combination performs best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First we need to set up everthing to run on AWS, namely the S3 bucket and IAM role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Define IAM role\n",
    "region = boto3.Session().region_name    \n",
    "smclient = boto3.Session().client(\"sagemaker\")\n",
    "role = get_execution_role()\n",
    "\n",
    "# S3 bucket\n",
    "bucket = '<my-bucket-name>'\n",
    "prefix = 'sagemaker/toxic-comments'\n",
    "\n",
    "# get the zipped training and test data from S3 (the zip already \n",
    "# contains differently preprocessed versions to save some time) \n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3.Bucket(bucket).download_file(\"jigsaw-toxic-comment-classification-challenge.zip\",\n",
    "                                \"local-jigsaw-toxic-comment-classification-challenge.zip\")\n",
    "\n",
    "# unzip the data\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"local-jigsaw-toxic-comment-classification-challenge.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need several other libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.2.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (7.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (2.20.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (39.1.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (1.18.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.42.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.23)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (2.1.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from en_core_web_lg==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.20.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (39.1.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.6)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.23)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.42.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.1.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "# data processing and computation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "# NLP\n",
    "! pip install spacy\n",
    "import spacy\n",
    "! python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# IO, OS, time, etc.\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "# sagemaker\n",
    "from sagemaker.predictor import csv_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and prepare the data\n",
    "Depending on what models we'll use, preprocessing may differ, but let's first just have a exploratory look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53818</th>\n",
       "      <td>8fc72eb899ac9d19</td>\n",
       "      <td>Thanks! I have already been following your dis...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58754</th>\n",
       "      <td>9d5735c425d9b861</td>\n",
       "      <td>\"\\nThat IP is not in my range, and I don't kno...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35097</th>\n",
       "      <td>5db73cb28e44afe6</td>\n",
       "      <td>\"\\n\\n Definition of witchcraft/Wicca \\n\\nI hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101120</th>\n",
       "      <td>1d40bfa9dffeead1</td>\n",
       "      <td>\"\\n\\n The precise definition of race is under ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54815</th>\n",
       "      <td>927ea94607dafc8a</td>\n",
       "      <td>I can tell you from going through this storm t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121746</th>\n",
       "      <td>8b55b14570589862</td>\n",
       "      <td>Just received my first suggestion \\n\\nAnd abou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25292</th>\n",
       "      <td>42ec023d7d2332d7</td>\n",
       "      <td>Wrong 3D structure or wrong 2D structure\\n3D s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16718</th>\n",
       "      <td>2c186e6534619923</td>\n",
       "      <td>materials that I am entering as links.\\nI find...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35141</th>\n",
       "      <td>5dd5bb579c02127e</td>\n",
       "      <td>YOU RACIST! \\n\\nI'm gonna get the ACLU on you!...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63233</th>\n",
       "      <td>a9396c7be6dcafdf</td>\n",
       "      <td>\"\\n hey funk'  u allways say that: \"\"bla bla b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "53818   8fc72eb899ac9d19  Thanks! I have already been following your dis...   \n",
       "58754   9d5735c425d9b861  \"\\nThat IP is not in my range, and I don't kno...   \n",
       "35097   5db73cb28e44afe6  \"\\n\\n Definition of witchcraft/Wicca \\n\\nI hav...   \n",
       "101120  1d40bfa9dffeead1  \"\\n\\n The precise definition of race is under ...   \n",
       "54815   927ea94607dafc8a  I can tell you from going through this storm t...   \n",
       "121746  8b55b14570589862  Just received my first suggestion \\n\\nAnd abou...   \n",
       "25292   42ec023d7d2332d7  Wrong 3D structure or wrong 2D structure\\n3D s...   \n",
       "16718   2c186e6534619923  materials that I am entering as links.\\nI find...   \n",
       "35141   5dd5bb579c02127e  YOU RACIST! \\n\\nI'm gonna get the ACLU on you!...   \n",
       "63233   a9396c7be6dcafdf  \"\\n hey funk'  u allways say that: \"\"bla bla b...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "53818       0             0        0       0       0              0  \n",
       "58754       0             0        0       0       0              0  \n",
       "35097       0             0        0       0       0              0  \n",
       "101120      0             0        0       0       0              0  \n",
       "54815       0             0        0       0       0              0  \n",
       "121746      0             0        0       0       0              0  \n",
       "25292       0             0        0       0       0              0  \n",
       "16718       0             0        0       0       0              0  \n",
       "35141       1             0        0       0       0              0  \n",
       "63233       1             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the training data\n",
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      "id               159571 non-null object\n",
      "comment_text     159571 non-null object\n",
      "toxic            159571 non-null int64\n",
      "severe_toxic     159571 non-null int64\n",
      "obscene          159571 non-null int64\n",
      "threat           159571 non-null int64\n",
      "insult           159571 non-null int64\n",
      "identity_hate    159571 non-null int64\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the flags for the types of toxicity are encode with 1 if that type appears and with 0 if not. There are 159571 training examples and no missing values. Let's look at some of the clean and some of the toxic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A non-toxic comment:\n",
      " Main Page \n",
      "\n",
      "I think whoever is really writing this article should try to get it featured on the main page before the election, because after the election who cares? \n",
      "\n",
      "A toxic comment:\n",
      " Cunts\n",
      "\n",
      "Way to discuss, eh?  Protect the article and then fuck it up.  Despicable.  Well if you're the kind of people who think there is some special meaning to being blown up in a Montego, I haven't got the fucking time to argue.  190.46.108.141\n"
     ]
    }
   ],
   "source": [
    "print(\"A non-toxic comment:\\n\",train_df[train_df.sum(axis=1)==0].comment_text.iloc[1234],\"\\n\")\n",
    "print(\"A toxic comment:\\n\",train_df[train_df.sum(axis=1)>0].comment_text.iloc[1234])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most common words used in toxic comments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common toxic word counts:\n",
      "fuck         8715\n",
      "like         3802\n",
      "shit         3687\n",
      "wikipedia    3637\n",
      "nigger       3304\n",
      "fucking      3256\n",
      "suck         3052\n",
      "ass          2718\n",
      "hate         2656\n",
      "u            2636\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# list of toxic words\n",
    "toxic_words = []\n",
    "\n",
    "# standardize the words by removing stop words, numbers, punctuation, etc.\n",
    "doc_generator = nlp.pipe(train_df[train_df.sum(axis=1)>0].comment_text.values.tolist())\n",
    "for doc in doc_generator:\n",
    "    for tok in doc:\n",
    "        if ((not tok.is_stop) and \n",
    "        (not tok.is_punct) and \n",
    "        (not tok.like_num) and \n",
    "        not tok.text in [\"\\n\",\"\\n\\n\",\"|\",\" \",\"=\"]):\n",
    "            toxic_words.append(tok.lower_)\n",
    "\n",
    "print(f\"Common toxic word counts:\\n{pd.Series(toxic_words).value_counts().head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mmmh, clearly some nasty language in there. Let's have a look at how the labels are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_toxic 143346\n",
      "toxic            15294\n",
      "severe_toxic      1595\n",
      "obscene           8449\n",
      "threat             478\n",
      "insult            7877\n",
      "identity_hate     1405\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# look at class imbalance\n",
    "non_toxic = len(train_df[train_df[train_df.columns[2:]] == 0][train_df.columns[2:]].dropna())\n",
    "print(\"non_toxic\",non_toxic)\n",
    "toxic = train_df.loc[:,\"toxic\":\"identity_hate\"].sum()\n",
    "print(toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution\n",
    "sns.barplot(\n",
    "    x=[\"non_toxic\"]+toxic.index.values.tolist(), y=[non_toxic]+toxic.values.tolist()\n",
    ")\n",
    "plt.gca().set_title(\"class disttribution - training set\")\n",
    "plt.gca().set_ylabel(\"count\")\n",
    "plt.gca().set_xlabel(\"classification\",rotation=0.2)\n",
    "_ = plt.gca().set_xticklabels(plt.gca().get_xticklabels(),rotation=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that the class distribution is quite imbalanced. Most comments are non-toxic and within the toxcicity categories, the distribution varies as well. Let's check that for the test data as well. There are many instances labeled with -1 which haven't been used for scoring in the competition, so we'll remove those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38913</th>\n",
       "      <td>4090db6be4eac774</td>\n",
       "      <td>****How long before the block is lifted? The I...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79949</th>\n",
       "      <td>856cc87ab4fb1a5d</td>\n",
       "      <td>You Like Dick.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151158</th>\n",
       "      <td>fca8c96ff0aee8c7</td>\n",
       "      <td>}} \\n {{course assignment | course = Education...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53400</th>\n",
       "      <td>58c04342e402610c</td>\n",
       "      <td>== New European vector maps == \\n\\n You're inv...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86489</th>\n",
       "      <td>904d3119cc70ff94</td>\n",
       "      <td>== Damnable liar == \\n You, sir are a damnable...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "38913   4090db6be4eac774  ****How long before the block is lifted? The I...   \n",
       "79949   856cc87ab4fb1a5d                                     You Like Dick.   \n",
       "151158  fca8c96ff0aee8c7  }} \\n {{course assignment | course = Education...   \n",
       "53400   58c04342e402610c  == New European vector maps == \\n\\n You're inv...   \n",
       "86489   904d3119cc70ff94  == Damnable liar == \\n You, sir are a damnable...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "38913       0             0        0       0       0              0  \n",
       "79949       0             0        0       0       0              0  \n",
       "151158      0             0        0       0       0              0  \n",
       "53400      -1            -1       -1      -1      -1             -1  \n",
       "86489      -1            -1       -1      -1      -1             -1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the test data\n",
    "test_df = pd.read_csv(\"./data/test.csv\").merge(pd.read_csv(\"./data/test_labels.csv\"),left_on=\"id\", right_on=\"id\")\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53118</th>\n",
       "      <td>5839501a904661ae</td>\n",
       "      <td>You're a silly little sock, darwinbish. Actual...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27681</th>\n",
       "      <td>2e1985336b8eaaa2</td>\n",
       "      <td>… … Apologies, I'm wrong about the photo, but ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94545</th>\n",
       "      <td>9db8a9b190fc7e88</td>\n",
       "      <td>::::You are factually incorrect, I am not the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96732</th>\n",
       "      <td>a1675202dbaf46c2</td>\n",
       "      <td>\" 2014 (UTC) \\n : I have just removed a furthe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39386</th>\n",
       "      <td>4159cadf6c3cfa1c</td>\n",
       "      <td>== ITN for Charles Mackerras ==</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                       comment_text  \\\n",
       "53118  5839501a904661ae  You're a silly little sock, darwinbish. Actual...   \n",
       "27681  2e1985336b8eaaa2  … … Apologies, I'm wrong about the photo, but ...   \n",
       "94545  9db8a9b190fc7e88  ::::You are factually incorrect, I am not the ...   \n",
       "96732  a1675202dbaf46c2  \" 2014 (UTC) \\n : I have just removed a furthe...   \n",
       "39386  4159cadf6c3cfa1c                    == ITN for Charles Mackerras ==   \n",
       "\n",
       "       toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "53118      1             0        0       0       0              0  \n",
       "27681      0             0        0       0       0              0  \n",
       "94545      0             0        0       0       0              0  \n",
       "96732      0             0        0       0       0              0  \n",
       "39386      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the -1 labeled instance (will be converted to float so convert back to int)\n",
    "test_df = test_df.where(test_df!=-1).dropna()\n",
    "test_df[test_df.columns[2:]] = test_df[test_df.columns[2:]].astype(np.int)\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_toxic 57528\n",
      "toxic            6090\n",
      "severe_toxic      367\n",
      "obscene          3691\n",
      "threat            211\n",
      "insult           3427\n",
      "identity_hate     712\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# look at class imbalance\n",
    "non_toxic = len(test_df[train_df[test_df.columns[2:]] == 0][test_df.columns[2:]].dropna())\n",
    "print(\"non_toxic\",non_toxic)\n",
    "toxic = test_df.loc[:,\"toxic\":\"identity_hate\"].sum()\n",
    "print(toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distibution \n",
    "sns.barplot(\n",
    "    x=[\"non_toxic\"]+toxic.index.values.tolist(), y=[non_toxic]+toxic.values.tolist()\n",
    ")\n",
    "plt.gca().set_title(\"class disttribution - test set\")\n",
    "plt.gca().set_ylabel(\"count\")\n",
    "plt.gca().set_xlabel(\"classification\",rotation=0.2)\n",
    "_ = plt.gca().set_xticklabels(plt.gca().get_xticklabels(),rotation=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning up we have\n",
      "159571 training examples (71.38%)\n",
      "63978 test examples (28.62%)\n"
     ]
    }
   ],
   "source": [
    "num_train = len(train_df)\n",
    "num_test = len(test_df)\n",
    "print(f\"After cleaning up we have\\n{num_train} training examples ({num_train/(num_train+num_test)*100:.2f}%)\\n{len(test_df)} test examples ({num_test/(num_train+num_test)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the distributions between training and test examples are very similar. The test set constitutes a bit less than a third to all examples. Note that there are comments which are flagged for multiple types of toxicity, some were even flagged for all six types!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of toxicity labels per comment for all training examples:\n",
      " 0    143346\n",
      "1      6360\n",
      "3      4209\n",
      "2      3480\n",
      "4      1760\n",
      "5       385\n",
      "6        31\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "multi_toxic = train_df.sum(axis=1).value_counts()\n",
    "print(\"Count of toxicity labels per comment for all training examples:\\n\",multi_toxic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an overview over how the labels are distributed, we can start preprocessing the data. When browsing through the comments, we will notice that some of the comment texts contain elements such as IP addresses, hyper-links, or user names. Such elements can potentially introduce data leakage, which means it provides information that should not actually be available in the general application case. Information that may leak from the training data into the test data or future examples but has actually nothing to do with the problem (toxic text in this case). The problem of leaky features can lead to severe overfitting during development of a model and then nasty surprises when deploying the model. Let's have a look a potentially leaky elements in the text here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potentially leaky elements in the training set comment texts:\n",
      "IP ip addresses found: 11257\n",
      "Hyper-links found: 6443\n",
      "User names found: 233\n",
      "\n",
      "Potentially leaky elements in the test set comment texts:\n",
      "IP ip addresses found: 412\n",
      "Hyper-links found: 2325\n",
      "User names found: 39\n",
      "\n",
      "Common elements in the training and test sets:\n",
      "IP ip addresses found: 13\n",
      "Hyper-links found: 6\n",
      "User names found: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join all text together\n",
    "training_text = \"\\n\".join(train_df.comment_text.values)\n",
    "test_text = \"\\n\".join(test_df.comment_text.values)\n",
    "\n",
    "# IP addresses\n",
    "train_ips = re.findall(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",training_text)\n",
    "test_ips = re.findall(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",test_text)\n",
    "common_ips = set(train_ips).intersection(set(test_ips))\n",
    "\n",
    "# Hyperlinks\n",
    "train_links = re.findall(\"http://.*\\..*\",training_text)\n",
    "test_links = re.findall(\"http://.*\\..*\",test_text)\n",
    "common_links = set(train_links).intersection(set(test_links))\n",
    "\n",
    "# User names\n",
    "train_users = re.findall(\"\\[\\[User.*\\|\",training_text)\n",
    "test_users = re.findall(\"\\[\\[User.*\\|\",test_text)\n",
    "common_users = set(train_users).intersection(set(test_users))\n",
    "\n",
    "# print results\n",
    "print(f\"Potentially leaky elements in the training set comment texts:\\n\\\n",
    "IP ip addresses found: {len(train_ips)}\\n\\\n",
    "Hyper-links found: {len(train_links)}\\n\\\n",
    "User names found: {len(train_users)}\\n\\n\\\n",
    "Potentially leaky elements in the test set comment texts:\\n\\\n",
    "IP ip addresses found: {len(test_ips)}\\n\\\n",
    "Hyper-links found: {len(test_links)}\\n\\\n",
    "User names found: {len(test_users)}\\n\\n\\\n",
    "Common elements in the training and test sets:\\n\\\n",
    "IP ip addresses found: {len(common_ips)}\\n\\\n",
    "Hyper-links found: {len(common_links)}\\n\\\n",
    "User names found: {len(common_users)}\\n\\n\\\n",
    "\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some of these elements do indeed leak from the training into the test data. We should hence remove these elements. Let's build a transformer class that will be doing the preprocessing and can be integrated in Sci-kit Learn's pipelines. We will also use spaCy's preprocessing functions to performs steps like removal of punctuation, numbers, stop words and doing lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# a transformer class to preprocess the comments\n",
    "class CommentPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lemmatize=False, stopwords=False, no_num=True, no_punct=True):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.stopwords = stopwords\n",
    "        self.no_num = no_num\n",
    "        self.no_punct = no_punct\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        # lower case and remove leaky elements\n",
    "        def preprep(x):\n",
    "            x = x.lower()\n",
    "            x = re.sub(\"\\\\n\",\" \",x)\n",
    "            x = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",x)\n",
    "            x = re.sub(\"http://.*\\..*\",\"\",x)\n",
    "            x = re.sub(\"\\[\\[User.*\\|\",\"\",x)\n",
    "            \n",
    "            # tokenize the comment\n",
    "            tokens = [tok for tok in nlp(x)]\n",
    "            \n",
    "            # if chose, remove numbers, punctuation, stopwords, and lemmatize\n",
    "            if self.no_num:\n",
    "                tokens = [tok for tok in tokens if not tok.like_num]\n",
    "            if self.no_punct:\n",
    "                tokens = [tok for tok in tokens if not tok.is_punct]\n",
    "            if self.stopwords:\n",
    "                tokens = [tok for tok in tokens if not tok.is_stop]\n",
    "            if self.lemmatize:\n",
    "                tokens = [tok.lemma_ for tok in tokens]\n",
    "            else:\n",
    "                tokens = [tok.text for tok in tokens]\n",
    "\n",
    "            # join the text back together and return\n",
    "            x = \" \".join(tokens)\n",
    "            return(x)\n",
    "\n",
    "        return X.apply(preprep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create some versions with different preprocessing options, namely applying optional stop word removal and/or lemmatization, and (because the preprocessing takes a long time) save them to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctution and numbers\n",
    "# (since this takes a while, I've already prepared the preprocessed CSVs\n",
    "# and I commented out the code below, uncomment it if you want to do the \n",
    "# preprocessing yourself)\n",
    "_ = \"\"\"X_train = CommentPreprocessor(lemmatize=False, \n",
    "                              stopwords=False, \n",
    "                              no_num=True, \n",
    "                              no_punct=True).transform(train_df.comment_text)\n",
    "X_train.to_csv(\"./data/X_train.csv\",header=True,index=False)\n",
    "\n",
    "X_test = CommentPreprocessor(lemmatize=False, \n",
    "                              stopwords=False, \n",
    "                              no_num=True, \n",
    "                              no_punct=True).transform(test_df.comment_text)\n",
    "X_test.to_csv(\"./data/X_test.csv\",header=True,index=False)\n",
    "\n",
    "# remove punctution, numbers, and stopwords\n",
    "X_train_stop = CommentPreprocessor(lemmatize=False, \n",
    "                                   stopwords=True, \n",
    "                                   no_num=True, \n",
    "                                   no_punct=True).transform(train_df.comment_text)\n",
    "X_train_stop.to_csv(\"./data/X_train_stop.csv\",header=True,index=False)\n",
    "\n",
    "X_test_stop = CommentPreprocessor(lemmatize=False, \n",
    "                                  stopwords=True, \n",
    "                                  no_num=True, \n",
    "                                  no_punct=True).transform(test_df.comment_text)\n",
    "X_test_stop.to_csv(\"./data/X_test_stop.csv\",header=True,index=False)\n",
    "\n",
    "# remove punctution and numbers and lemmatize\n",
    "X_train_lem = CommentPreprocessor(lemmatize=True, \n",
    "                                  stopwords=False, \n",
    "                                  no_num=True, \n",
    "                                  no_punct=True).transform(train_df.comment_text)\n",
    "X_train_lem.to_csv(\"./data/X_train_lem.csv\",header=True,index=False)\n",
    "\n",
    "X_test_lem = CommentPreprocessor(lemmatize=True, \n",
    "                                 stopwords=False, \n",
    "                                 no_num=True, \n",
    "                                 no_punct=True).transform(test_df.comment_text)\n",
    "X_test_lem.to_csv(\"./data/X_test_lem.csv\",header=True,index=False)\n",
    "\n",
    "# remove punctution, numbers, and stopwords\n",
    "X_train_stop_lem = CommentPreprocessor(lemmatize=True, \n",
    "                                       stopwords=True, \n",
    "                                       no_num=True, \n",
    "                                       no_punct=True).transform(train_df.comment_text)\n",
    "X_train_stop_lem.to_csv(\"./data/X_train_stop_lem.csv\",header=True,index=False)\n",
    "\n",
    "X_test_stop_lem = CommentPreprocessor(lemmatize=True, \n",
    "                                      stopwords=True, \n",
    "                                      no_num=True, \n",
    "                                      no_punct=True).transform(test_df.comment_text)\n",
    "X_test_stop_lem.to_csv(\"./data/X_test_stop_lem.csv\",header=True,index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the preprocessed data from the newly created CSVs\n",
    "X_train = pd.read_csv(\"./data/X_train.csv\",skip_blank_lines=False)\n",
    "X_test = pd.read_csv(\"./data/X_test.csv\",skip_blank_lines=False)\n",
    "X_train_stop = pd.read_csv(\"./data/X_train_stop.csv\",skip_blank_lines=False)\n",
    "X_test_stop = pd.read_csv(\"./data/X_test_stop.csv\",skip_blank_lines=False)\n",
    "X_train_lem = pd.read_csv(\"./data/X_train_lem.csv\",skip_blank_lines=False)\n",
    "X_test_lem = pd.read_csv(\"./data/X_test_lem.csv\",skip_blank_lines=False)\n",
    "X_train_stop_lem = pd.read_csv(\"./data/X_train_stop_lem.csv\",skip_blank_lines=False)\n",
    "X_test_stop_lem = pd.read_csv(\"./data/X_test_stop_lem.csv\",skip_blank_lines=False)\n",
    "\n",
    "# get the labels\n",
    "y_train = train_df.iloc[:,2:].reset_index(drop=True).copy()\n",
    "y_test = test_df.iloc[:,2:].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the preprocessing, some of the comments got all their words removed and are now empty. This is, of course, useless for the classification task and we have to remove empty comments in the different training nd test sets and also remove the corresponding labels. We can see in the following that the the version with and without stop word removal have different empty comments, so we have to remove the empty comments from these two types of training and test sets indiviually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the train and the train_lem (and corresponding test sets are missing the same comments)\n",
    "idx_train = X_train[X_train.comment_text.isnull()].index\n",
    "idx_train_lem = X_train_lem[X_train_lem.comment_text.isnull()].index\n",
    "idx_test = X_test[X_test.comment_text.isnull()].index\n",
    "idx_test_lem = X_test_lem[X_test_lem.comment_text.isnull()].index\n",
    "assert idx_train_lem.all() == idx_train.all()\n",
    "assert idx_test_lem.all() == idx_test.all()\n",
    "\n",
    "# the train_stop and the train_stop_lem (and corresponding test sets are missing the same comments)\n",
    "idx_train_stop = X_train_stop[X_train_stop.comment_text.isnull()].index\n",
    "idx_train_stop_lem = X_train_stop_lem[X_train_stop_lem.comment_text.isnull()].index\n",
    "idx_test_stop = X_test_stop[X_test_stop.comment_text.isnull()].index\n",
    "idx_test_stop_lem = X_test_stop_lem[X_test_stop_lem.comment_text.isnull()].index\n",
    "assert idx_train_stop_lem.all() == idx_train_stop.all()\n",
    "assert idx_test_stop_lem.all() == idx_test_stop.all()\n",
    "\n",
    "# remove the empty comments\n",
    "X_train = X_train.drop(idx_train)\n",
    "X_test = X_test.drop(idx_test)\n",
    "X_train_stop = X_train_stop.drop(idx_train_stop)\n",
    "X_test_stop = X_test_stop.drop(idx_test_stop)\n",
    "X_train_lem = X_train_lem.drop(idx_train)\n",
    "X_test_lem = X_test_lem.drop(idx_test)\n",
    "X_train_stop_lem = X_train_stop_lem.drop(idx_train_stop)\n",
    "X_test_stop_lem = X_test_stop_lem.drop(idx_test_stop)\n",
    "\n",
    "# remove the corresponding labels\n",
    "y_train_stop = y_train.copy()\n",
    "y_train = y_train.drop(idx_train)\n",
    "y_train_stop = y_train_stop.drop(idx_train_stop)\n",
    "y_test_stop = y_test.copy()\n",
    "y_test = y_test.drop(idx_test)\n",
    "y_test_stop = y_test_stop.drop(idx_test_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model\n",
    "We will only proceed with the stop-word-free data set to keep it simple (and because I already know it performs best). We will apply TFIDF-Vecotrization to the data to turn the comment text into a numerical form, which can be processed by the algorithms. We will put the training data in a seperate directory which Sagemaker will also copy to a S3 bucket and use it as the standard training data location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tfidf-vectorization to the comment texts\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vec.fit_transform(X_train_stop.values.reshape(len(X_train_stop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory for training data\n",
    "WORK_DIRECTORY = \"train_data\"\n",
    "if not os.path.isdir(WORK_DIRECTORY):\n",
    "    os.mkdir(WORK_DIRECTORY)\n",
    "\n",
    "# write the trainingdata (features and labels) to CSV    \n",
    "scipy.sparse.save_npz(\"./\"+WORK_DIRECTORY+\"/X_train_stop.npz\", X_train_tfidf)\n",
    "y_train_stop.to_csv(\"./\"+WORK_DIRECTORY+\"/y_train_stop.csv\",header=True,index=False)\n",
    "\n",
    "# upload the data to S3 to be accessed for training later\n",
    "train_input = sagemaker.Session().upload_data(WORK_DIRECTORY, key_prefix=\"{}/{}\".format(prefix, WORK_DIRECTORY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we still have to choose a model and potentially it's hyperparamters. Multinomial and complement naive Bayes classifier are commonly used for text classification and also a logitsic regression might be an interesting candidate as one can tune the degree of regularization. The `sklearn` library has implementations for these algorithms and hyperparameter optimization but if we want to make use of AWS Sagemaker's training and hosting capabilities, we first have to containerize the `sklearn` code so that it can be used as a Sagemaker estimator. For that purpose we'll write a script that contains the corresponding code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script.py\n",
    "# write this notebook cell as a script file\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# the training function\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters could be described here. In this simple example we are performing a gridsearch \n",
    "    # in wich we have already specified the range of hyper-parameters, so we do not have to pass\n",
    "    # them as arguments\n",
    "    \n",
    "    # parser.add_argument('--some_hyperparam', type=str, default=None)\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Take the set of files and read them all into pandas dataframes\n",
    "    input_files = [ os.path.join(args.train, file) for file in sorted(os.listdir(args.train)) ]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "\n",
    "    # load the input data (TFIDF sparse matrix for the features, pandas dataframe for the labels)\n",
    "    X_tfidf = scipy.sparse.load_npz(input_files[0])\n",
    "    y_train = pd.read_csv(input_files[1], skip_blank_lines=False, engine=\"python\")\n",
    "\n",
    "\n",
    "    # paramter dictionary for the grid search\n",
    "    params = [{\"estimator\":[MultinomialNB(),\n",
    "                            ComplementNB()]},\n",
    "              {\"estimator\":[LogisticRegression(class_weight=\"balanced\",\n",
    "                                               solver=\"lbfgs\",\n",
    "                                               penalty=\"l2\")],\n",
    "               \"estimator__C\": np.logspace(-1,3,5)}]\n",
    "    \n",
    "    # perform the grid search\n",
    "    clf = GridSearchCV(estimator=OneVsRestClassifier(MultinomialNB()),\n",
    "                       param_grid=params,\n",
    "                       scoring=\"f1_macro\",\n",
    "                       n_jobs=1,\n",
    "                       cv=3,\n",
    "                       verbose=10).fit(X_tfidf,y_train)\n",
    "    \n",
    "    print(f\"Best F1 Macro score: {clf.best_score_}, best parameters: {clf.best_params_}\")\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(clf, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "\n",
    "# the inference function\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Deserialized and return fitted model\n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now that we have specified what sould be done during training in the script file, we can easily create a Sagemaker estimator from its prebuild Scikit-Learn container. We'll just specify the script's location and some, our role, the Sagemaker session, and on what type of instance we want to perform the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-27 11:52:48 Starting - Starting the training job...\n",
      "2020-01-27 11:52:51 Starting - Launching requested ML instances......\n",
      "2020-01-27 11:53:51 Starting - Preparing the instances for training...\n",
      "2020-01-27 11:54:43 Downloading - Downloading input data...\n",
      "2020-01-27 11:55:16 Training - Training image download completed. Training in progress...\u001b[34m2020-01-27 11:55:17,411 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2020-01-27 11:55:17,414 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-27 11:55:17,424 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-01-27 11:55:17,667 sagemaker-containers INFO     Module script does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-01-27 11:55:17,667 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-01-27 11:55:17,667 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-01-27 11:55:17,668 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: script\n",
      "  Building wheel for script (setup.py): started\n",
      "  Building wheel for script (setup.py): finished with status 'done'\n",
      "  Created wheel for script: filename=script-1.0.0-py2.py3-none-any.whl size=6757 sha256=bedd8e6e68a2d1503a2eb0ef1f131c6d517ea1919d4d5dd0d47f0716690d1dd0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-96shmuno/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built script\u001b[0m\n",
      "\u001b[34mInstalling collected packages: script\u001b[0m\n",
      "\u001b[34mSuccessfully installed script-1.0.0\u001b[0m\n",
      "\u001b[34m2020-01-27 11:55:18,933 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-27 11:55:18,943 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2020-01-27-11-52-48-228\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-627981590504/sagemaker-scikit-learn-2020-01-27-11-52-48-228/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"script.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-627981590504/sagemaker-scikit-learn-2020-01-27-11-52-48-228/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2020-01-27-11-52-48-228\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-627981590504/sagemaker-scikit-learn-2020-01-27-11-52-48-228/source/sourcedir.tar.gz\",\"module_name\":\"script\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m script\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mFitting 3 folds for each of 7 candidates, totalling 21 fits\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\u001b[0m\n",
      "\u001b[34m[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) \u001b[0m\n",
      "\u001b[34m[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), score=0.09348921252349533, total=   0.4s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\u001b[0m\n",
      "\u001b[34m[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) \u001b[0m\n",
      "\u001b[34m[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), score=0.09185739258692355, total=   0.4s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.2s remaining:    0.0s\u001b[0m\n",
      "\u001b[34m[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) \u001b[0m\n",
      "\u001b[34m[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), score=0.09760159254784623, total=   0.4s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.8s remaining:    0.0s\u001b[0m\n",
      "\u001b[34m[CV] estimator=ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False) \u001b[0m\n",
      "\u001b[34m[CV]  estimator=ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), score=0.3055931336763715, total=   0.4s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    2.4s remaining:    0.0s\u001b[0m\n",
      "\u001b[34m[CV] estimator=ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False) \u001b[0m\n",
      "\u001b[34m[CV]  estimator=ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), score=0.2974887668795152, total=   0.4s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    3.0s remaining:    0.0s\u001b[0m\n",
      "\u001b[34m[CV] estimator=ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False) \u001b[0m\n",
      "\u001b[34m[CV]  estimator=ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), score=0.3014261247517409, total=   0.4s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    3.6s remaining:    0.0s\u001b[0m\n",
      "\u001b[34m[CV] estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=0.1 \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[CV]  estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=0.1, score=0.5128645085469472, total=  23.9s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   27.6s remaining:    0.0s\u001b[0m\n",
      "\u001b[34m[CV] estimator=LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=0.1 \u001b[0m\n",
      "\u001b[34m[CV]  estimator=LogisticRegression(C=0.1, class_weight='balanced', dual=False,\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   52.5s remaining:    0.0s\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=0.1, score=0.5003003877788862, total=  24.8s\u001b[0m\n",
      "\u001b[34m[CV] estimator=LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=0.1 \u001b[0m\n",
      "\u001b[34m[CV]  estimator=LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=0.1, score=0.5059798932038683, total=  22.6s\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  1.3min remaining:    0.0s\u001b[0m\n",
      "\u001b[34m[CV] estimator=LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=1.0 \u001b[0m\n",
      "\u001b[34m[CV]  estimator=LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=1.0, score=0.5667827188361465, total=  41.9s\u001b[0m\n",
      "\u001b[34m[CV] estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=1.0 \u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m[CV]  estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=1.0, score=0.5516136047053706, total=  46.5s\u001b[0m\n",
      "\u001b[34m[CV] estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=1.0 \u001b[0m\n",
      "\u001b[34m[CV]  estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=1.0, score=0.5625541791323879, total=  48.4s\u001b[0m\n",
      "\u001b[34m[CV] estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=10.0 \u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m[CV]  estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=10.0, score=0.5977106940023099, total=  59.5s\u001b[0m\n",
      "\u001b[34m[CV] estimator=LogisticRegression(C=10.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=10.0 \u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m[CV]  estimator=LogisticRegression(C=10.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=10.0, score=0.5663574729165094, total= 1.0min\u001b[0m\n",
      "\u001b[34m[CV] estimator=LogisticRegression(C=10.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=10.0 \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m[CV]  estimator=LogisticRegression(C=10.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=10.0, score=0.5955948838020753, total=  58.0s\u001b[0m\n",
      "\u001b[34m[CV] estimator=LogisticRegression(C=10.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=100.0 \u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m[CV]  estimator=LogisticRegression(C=10.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=100.0, score=0.5657390498633958, total=  59.2s\u001b[0m\n",
      "\u001b[34m[CV] estimator=LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=100.0 \u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m[CV]  estimator=LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=100.0, score=0.5414935964190183, total= 1.0min\u001b[0m\n",
      "\u001b[34m[CV] estimator=LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=100.0 \u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m[CV]  estimator=LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=100.0, score=0.5716965611712251, total=  59.2s\u001b[0m\n",
      "\u001b[34m[CV] estimator=LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=1000.0 \u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m[CV]  estimator=LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=1000.0, score=0.5375225656199031, total=  60.0s\u001b[0m\n",
      "\u001b[34m[CV] estimator=LogisticRegression(C=1000.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=1000.0 \u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m[CV]  estimator=LogisticRegression(C=1000.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=1000.0, score=0.519283770225407, total= 1.0min\u001b[0m\n",
      "\u001b[34m[CV] estimator=LogisticRegression(C=1000.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=1000.0 \u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m[CV]  estimator=LogisticRegression(C=1000.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), estimator__C=1000.0, score=0.536495428746345, total= 1.0min\u001b[0m\n",
      "\u001b[34m[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed: 12.5min finished\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34mBest F1 Macro score: 0.5865544202720849, best parameters: {'estimator': LogisticRegression(C=10.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), 'estimator__C': 10.0}\u001b[0m\n",
      "\u001b[34m2020-01-27 12:09:05,162 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-01-27 12:09:16 Uploading - Uploading generated training model\n",
      "2020-01-27 12:09:16 Completed - Training job completed\n",
      "Training seconds: 873\n",
      "Billable seconds: 873\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "script_path = \"./script.py\"\n",
    "\n",
    "# create a SKLearn estimator\n",
    "sklearn_clf = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    hyperparameters=None)\n",
    "\n",
    "# start the training with specifying the S3 location with the training data\n",
    "sklearn_clf.fit({'train': train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "If we now want to use the trained model, we'll first have to deploy it to an endpoint instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "# either we create a predictor by directly deploying the model we just trained to an endpoint\n",
    "predictor = sklearn_clf.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")\n",
    "\n",
    "# or in case we want to access the trained model later and not run the training job again,\n",
    "# we can take the model output from S3 and the image of the model and deploy it \n",
    "#sklearn_clf = sagemaker.model.Model(\n",
    "#    model_data=\"<model-data-location>\"\n",
    "#    image=\"<image-location>\",\n",
    "#    role=role)\n",
    "\n",
    "#predictor = sklearn_clf.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll also tfidf-vectorize the test data with the previously trained vectorizer so that we can feed it to the algorithm and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tfidf_vec.transform(X_test_stop.values.reshape(len(X_test_stop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions (because of the endpoints limited capacity, I'll only check 1000 examples)\n",
    "y_pred = [predictor.predict(X_test_tfidf[i].toarray()) for i in range(1000)]\n",
    "y_pred = np.concatenate(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making the predictions on the test data, we don't need the endpoint anymore and we should clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_clf.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our final model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5469036499716472\n",
      "Accuracy: 0.839\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "f1 = f1_score(y_test_stop[:1000],y_pred,average=\"macro\")\n",
    "acc = accuracy_score(y_test_stop[:1000],y_pred)\n",
    "\n",
    "print(f\"F1 score: {f1}\\nAccuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy is not bad, but because of the strong class balance, accuracy is not a very suitable metric. Most comments are non-toxic and we'd get a decent accuracy just by always predicting the majority class. The macro averaged F1 score, on the other hand, considers both false positives and false negatives and averages equal over all classes without accounting for the imbalance. The value looks somewhat low but considering that it counts in minority classes (which are harder to predict due to their rare occurrence in the training data) with the same weights as majority classes, it's actually not a horrible score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We used shallow, non-sequential machine learning methods to classify Wikipedia article comments according to their verbal toxicity, using AWS Sagemakers capabilities of buliding, training and hosting models. \n",
    "\n",
    "In the [next](./02_ToxicDLOnAWS.ipynb) notebook we'll tackle the exact same task again but with employing sequential deep learning methods such as 1DCov and LSTM neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
